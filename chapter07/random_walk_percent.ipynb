{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# all states\n",
    "N_STATES = 19\n",
    "\n",
    "# discount\n",
    "GAMMA = 1\n",
    "\n",
    "# all states but terminal states\n",
    "STATES = np.arange(1, N_STATES + 1)\n",
    "\n",
    "# start from the middle state\n",
    "START_STATE = 10\n",
    "\n",
    "# two terminal states\n",
    "# an action leading to the left terminal state has reward -1\n",
    "# an action leading to the right terminal state has reward 1\n",
    "END_STATES = [0, N_STATES + 1]\n",
    "\n",
    "# true state value from bellman equation\n",
    "TRUE_VALUE = np.arange(-20, 22, 2) / 20.0\n",
    "TRUE_VALUE[0] = TRUE_VALUE[-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# n-steps TD method\n",
    "# @value: values for each state, will be updated\n",
    "# @n: # of steps\n",
    "# @alpha: # step size\n",
    "def temporal_difference(value, n, alpha):\n",
    "    # initial starting state\n",
    "    state = START_STATE\n",
    "\n",
    "    # arrays to store states and rewards for an episode\n",
    "    # space isn't a major consideration, so I didn't use the mod trick\n",
    "    states = [state]\n",
    "    rewards = [0]\n",
    "\n",
    "    # track the time\n",
    "    time = 0\n",
    "\n",
    "    # the length of this episode\n",
    "    T = float('inf')\n",
    "    while True:\n",
    "        # go to next time step\n",
    "        time += 1\n",
    "\n",
    "        if time < T:\n",
    "            # choose an action randomly\n",
    "            if np.random.binomial(1, 0.5) == 1:\n",
    "                next_state = state + 1\n",
    "            else:\n",
    "                next_state = state - 1\n",
    "\n",
    "            if next_state == 0:\n",
    "                reward = -1\n",
    "            elif next_state == 20:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "            # store new state and new reward\n",
    "            states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if next_state in END_STATES:\n",
    "                T = time\n",
    "\n",
    "        # get the time of the state to update\n",
    "        update_time = time - n\n",
    "        if update_time >= 0:\n",
    "            returns = 0.0\n",
    "            # calculate corresponding rewards\n",
    "            for t in range(update_time + 1, min(T, update_time + n) + 1):\n",
    "                returns += pow(GAMMA, t - update_time - 1) * rewards[t]\n",
    "            # add state value to the return\n",
    "            if update_time + n <= T:\n",
    "                returns += pow(GAMMA, n) * value[states[(update_time + n)]]\n",
    "            state_to_update = states[update_time]\n",
    "            # update the state value\n",
    "            if not state_to_update in END_STATES:\n",
    "                value[state_to_update] += alpha * (returns - value[state_to_update])\n",
    "        if update_time == T - 1:\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Figure 7.2, it will take quite a while\n",
    "def figure7_2():\n",
    "    # all possible steps\n",
    "    steps = np.power(2, np.arange(0, 10))\n",
    "\n",
    "    # all possible alphas\n",
    "    alphas = np.arange(0, 1.1, 0.1)\n",
    "\n",
    "    # each run has 10 episodes\n",
    "    episodes = 10\n",
    "\n",
    "    # perform 100 independent runs\n",
    "    runs = 100\n",
    "\n",
    "    # track the errors for each (step, alpha) combination\n",
    "    errors = np.zeros((len(steps), len(alphas)))\n",
    "    for run in tqdm(range(0, runs)):\n",
    "        for step_ind, step in enumerate(steps):\n",
    "            for alpha_ind, alpha in enumerate(alphas):\n",
    "                # print('run:', run, 'step:', step, 'alpha:', alpha)\n",
    "                value = np.zeros(N_STATES + 2)\n",
    "                for ep in range(0, episodes):\n",
    "                    temporal_difference(value, step, alpha)\n",
    "                    # calculate the RMS error\n",
    "                    errors[step_ind, alpha_ind] += np.sqrt(np.sum(np.power(value - TRUE_VALUE, 2)) / N_STATES)\n",
    "    # take average\n",
    "    errors /= episodes * runs\n",
    "\n",
    "    for i in range(0, len(steps)):\n",
    "        plt.plot(alphas, errors[i, :], label='n = %d' % (steps[i]))\n",
    "    plt.xlabel('alpha')\n",
    "    plt.ylabel('RMS error')\n",
    "    plt.ylim([0.25, 0.55])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "figure7_2()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
