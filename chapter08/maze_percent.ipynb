{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "from copy import deepcopy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PriorityQueue:\n",
    "    def __init__(self):\n",
    "        self.pq = []\n",
    "        self.entry_finder = {}\n",
    "        self.REMOVED = '<removed-task>'\n",
    "        self.counter = 0\n",
    "\n",
    "    def add_item(self, item, priority=0):\n",
    "        if item in self.entry_finder:\n",
    "            self.remove_item(item)\n",
    "        entry = [priority, self.counter, item]\n",
    "        self.counter += 1\n",
    "        self.entry_finder[item] = entry\n",
    "        heapq.heappush(self.pq, entry)\n",
    "\n",
    "    def remove_item(self, item):\n",
    "        entry = self.entry_finder.pop(item)\n",
    "        entry[-1] = self.REMOVED\n",
    "\n",
    "    def pop_item(self):\n",
    "        while self.pq:\n",
    "            priority, count, item = heapq.heappop(self.pq)\n",
    "            if item is not self.REMOVED:\n",
    "                del self.entry_finder[item]\n",
    "                return item, priority\n",
    "        raise KeyError('pop from an empty priority queue')\n",
    "\n",
    "    def empty(self):\n",
    "        return not self.entry_finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# A wrapper class for a maze, containing all the information about the maze.\n",
    "# Basically it's initialized to DynaMaze by default, however it can be easily adapted\n",
    "# to other maze\n",
    "class Maze:\n",
    "    def __init__(self):\n",
    "        # maze width\n",
    "        self.WORLD_WIDTH = 9\n",
    "\n",
    "        # maze height\n",
    "        self.WORLD_HEIGHT = 6\n",
    "\n",
    "        # all possible actions\n",
    "        self.ACTION_UP = 0\n",
    "        self.ACTION_DOWN = 1\n",
    "        self.ACTION_LEFT = 2\n",
    "        self.ACTION_RIGHT = 3\n",
    "        self.actions = [self.ACTION_UP, self.ACTION_DOWN, self.ACTION_LEFT, self.ACTION_RIGHT]\n",
    "\n",
    "        # start state\n",
    "        self.START_STATE = [2, 0]\n",
    "\n",
    "        # goal state\n",
    "        self.GOAL_STATES = [[0, 8]]\n",
    "\n",
    "        # all obstacles\n",
    "        self.obstacles = [[1, 2], [2, 2], [3, 2], [0, 7], [1, 7], [2, 7], [4, 5]]\n",
    "        self.old_obstacles = None\n",
    "        self.new_obstacles = None\n",
    "\n",
    "        # time to change obstacles\n",
    "        self.obstacle_switch_time = None\n",
    "\n",
    "        # initial state action pair values\n",
    "        # self.stateActionValues = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.actions)))\n",
    "\n",
    "        # the size of q value\n",
    "        self.q_size = (self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.actions))\n",
    "\n",
    "        # max steps\n",
    "        self.max_steps = float('inf')\n",
    "\n",
    "        # track the resolution for this maze\n",
    "        self.resolution = 1\n",
    "\n",
    "    # extend a state to a higher resolution maze\n",
    "    # @state: state in lower resoultion maze\n",
    "    # @factor: extension factor, one state will become factor^2 states after extension\n",
    "    def extend_state(self, state, factor):\n",
    "        new_state = [state[0] * factor, state[1] * factor]\n",
    "        new_states = []\n",
    "        for i in range(0, factor):\n",
    "            for j in range(0, factor):\n",
    "                new_states.append([new_state[0] + i, new_state[1] + j])\n",
    "        return new_states\n",
    "\n",
    "    # extend a state into higher resolution\n",
    "    # one state in original maze will become @factor^2 states in @return new maze\n",
    "    def extend_maze(self, factor):\n",
    "        new_maze = Maze()\n",
    "        new_maze.WORLD_WIDTH = self.WORLD_WIDTH * factor\n",
    "        new_maze.WORLD_HEIGHT = self.WORLD_HEIGHT * factor\n",
    "        new_maze.START_STATE = [self.START_STATE[0] * factor, self.START_STATE[1] * factor]\n",
    "        new_maze.GOAL_STATES = self.extend_state(self.GOAL_STATES[0], factor)\n",
    "        new_maze.obstacles = []\n",
    "        for state in self.obstacles:\n",
    "            new_maze.obstacles.extend(self.extend_state(state, factor))\n",
    "        new_maze.q_size = (new_maze.WORLD_HEIGHT, new_maze.WORLD_WIDTH, len(new_maze.actions))\n",
    "        # new_maze.stateActionValues = np.zeros((new_maze.WORLD_HEIGHT, new_maze.WORLD_WIDTH, len(new_maze.actions)))\n",
    "        new_maze.resolution = factor\n",
    "        return new_maze\n",
    "\n",
    "    # take @action in @state\n",
    "    # @return: [new state, reward]\n",
    "    def step(self, state, action):\n",
    "        x, y = state\n",
    "        if action == self.ACTION_UP:\n",
    "            x = max(x - 1, 0)\n",
    "        elif action == self.ACTION_DOWN:\n",
    "            x = min(x + 1, self.WORLD_HEIGHT - 1)\n",
    "        elif action == self.ACTION_LEFT:\n",
    "            y = max(y - 1, 0)\n",
    "        elif action == self.ACTION_RIGHT:\n",
    "            y = min(y + 1, self.WORLD_WIDTH - 1)\n",
    "        if [x, y] in self.obstacles:\n",
    "            x, y = state\n",
    "        if [x, y] in self.GOAL_STATES:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = 0.0\n",
    "        return [x, y], reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# a wrapper class for parameters of dyna algorithms\n",
    "class DynaParams:\n",
    "    def __init__(self):\n",
    "        # discount\n",
    "        self.gamma = 0.95\n",
    "\n",
    "        # probability for exploration\n",
    "        self.epsilon = 0.1\n",
    "\n",
    "        # step size\n",
    "        self.alpha = 0.1\n",
    "\n",
    "        # weight for elapsed time\n",
    "        self.time_weight = 0\n",
    "\n",
    "        # n-step planning\n",
    "        self.planning_steps = 5\n",
    "\n",
    "        # average over several independent runs\n",
    "        self.runs = 10\n",
    "\n",
    "        # algorithm names\n",
    "        self.methods = ['Dyna-Q', 'Dyna-Q+']\n",
    "\n",
    "        # threshold for priority queue\n",
    "        self.theta = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# choose an action based on epsilon-greedy algorithm\n",
    "def choose_action(state, q_value, maze, dyna_params):\n",
    "    if np.random.binomial(1, dyna_params.epsilon) == 1:\n",
    "        return np.random.choice(maze.actions)\n",
    "    else:\n",
    "        values = q_value[state[0], state[1], :]\n",
    "        return np.random.choice([action for action, value in enumerate(values) if value == np.max(values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Trivial model for planning in Dyna-Q\n",
    "class TrivialModel:\n",
    "    # @rand: an instance of np.random.RandomState for sampling\n",
    "    def __init__(self, rand=np.random):\n",
    "        self.model = dict()\n",
    "        self.rand = rand\n",
    "\n",
    "    # feed the model with previous experience\n",
    "    def feed(self, state, action, next_state, reward):\n",
    "        state = deepcopy(state)\n",
    "        next_state = deepcopy(next_state)\n",
    "        if tuple(state) not in self.model.keys():\n",
    "            self.model[tuple(state)] = dict()\n",
    "        self.model[tuple(state)][action] = [list(next_state), reward]\n",
    "\n",
    "    # randomly sample from previous experience\n",
    "    def sample(self):\n",
    "        state_index = self.rand.choice(range(len(self.model.keys())))\n",
    "        state = list(self.model)[state_index]\n",
    "        action_index = self.rand.choice(range(len(self.model[state].keys())))\n",
    "        action = list(self.model[state])[action_index]\n",
    "        next_state, reward = self.model[state][action]\n",
    "        state = deepcopy(state)\n",
    "        next_state = deepcopy(next_state)\n",
    "        return list(state), action, list(next_state), reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Time-based model for planning in Dyna-Q+\n",
    "class TimeModel:\n",
    "    # @maze: the maze instance. Indeed it's not very reasonable to give access to maze to the model.\n",
    "    # @timeWeight: also called kappa, the weight for elapsed time in sampling reward, it need to be small\n",
    "    # @rand: an instance of np.random.RandomState for sampling\n",
    "    def __init__(self, maze, time_weight=1e-4, rand=np.random):\n",
    "        self.rand = rand\n",
    "        self.model = dict()\n",
    "\n",
    "        # track the total time\n",
    "        self.time = 0\n",
    "\n",
    "        self.time_weight = time_weight\n",
    "        self.maze = maze\n",
    "\n",
    "    # feed the model with previous experience\n",
    "    def feed(self, state, action, next_state, reward):\n",
    "        state = deepcopy(state)\n",
    "        next_state = deepcopy(next_state)\n",
    "        self.time += 1\n",
    "        if tuple(state) not in self.model.keys():\n",
    "            self.model[tuple(state)] = dict()\n",
    "\n",
    "            # Actions that had never been tried before from a state were allowed to be considered in the planning step\n",
    "            for action_ in self.maze.actions:\n",
    "                if action_ != action:\n",
    "                    # Such actions would lead back to the same state with a reward of zero\n",
    "                    # Notice that the minimum time stamp is 1 instead of 0\n",
    "                    self.model[tuple(state)][action_] = [list(state), 0, 1]\n",
    "\n",
    "        self.model[tuple(state)][action] = [list(next_state), reward, self.time]\n",
    "\n",
    "    # randomly sample from previous experience\n",
    "    def sample(self):\n",
    "        state_index = self.rand.choice(range(len(self.model.keys())))\n",
    "        state = list(self.model)[state_index]\n",
    "        action_index = self.rand.choice(range(len(self.model[state].keys())))\n",
    "        action = list(self.model[state])[action_index]\n",
    "        next_state, reward, time = self.model[state][action]\n",
    "\n",
    "        # adjust reward with elapsed time since last vist\n",
    "        reward += self.time_weight * np.sqrt(self.time - time)\n",
    "\n",
    "        state = deepcopy(state)\n",
    "        next_state = deepcopy(next_state)\n",
    "\n",
    "        return list(state), action, list(next_state), reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Model containing a priority queue for Prioritized Sweeping\n",
    "class PriorityModel(TrivialModel):\n",
    "    def __init__(self, rand=np.random):\n",
    "        TrivialModel.__init__(self, rand)\n",
    "        # maintain a priority queue\n",
    "        self.priority_queue = PriorityQueue()\n",
    "        # track predecessors for every state\n",
    "        self.predecessors = dict()\n",
    "\n",
    "    # add a @state-@action pair into the priority queue with priority @priority\n",
    "    def insert(self, priority, state, action):\n",
    "        # note the priority queue is a minimum heap, so we use -priority\n",
    "        self.priority_queue.add_item((tuple(state), action), -priority)\n",
    "\n",
    "    # @return: whether the priority queue is empty\n",
    "    def empty(self):\n",
    "        return self.priority_queue.empty()\n",
    "\n",
    "    # get the first item in the priority queue\n",
    "    def sample(self):\n",
    "        (state, action), priority = self.priority_queue.pop_item()\n",
    "        next_state, reward = self.model[state][action]\n",
    "        state = deepcopy(state)\n",
    "        next_state = deepcopy(next_state)\n",
    "        return -priority, list(state), action, list(next_state), reward\n",
    "\n",
    "    # feed the model with previous experience\n",
    "    def feed(self, state, action, next_state, reward):\n",
    "        state = deepcopy(state)\n",
    "        next_state = deepcopy(next_state)\n",
    "        TrivialModel.feed(self, state, action, next_state, reward)\n",
    "        if tuple(next_state) not in self.predecessors.keys():\n",
    "            self.predecessors[tuple(next_state)] = set()\n",
    "        self.predecessors[tuple(next_state)].add((tuple(state), action))\n",
    "\n",
    "    # get all seen predecessors of a state @state\n",
    "    def predecessor(self, state):\n",
    "        if tuple(state) not in self.predecessors.keys():\n",
    "            return []\n",
    "        predecessors = []\n",
    "        for state_pre, action_pre in list(self.predecessors[tuple(state)]):\n",
    "            predecessors.append([list(state_pre), action_pre, self.model[state_pre][action_pre][1]])\n",
    "        return predecessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# play for an episode for Dyna-Q algorithm\n",
    "# @q_value: state action pair values, will be updated\n",
    "# @model: model instance for planning\n",
    "# @maze: a maze instance containing all information about the environment\n",
    "# @dyna_params: several params for the algorithm\n",
    "def dyna_q(q_value, model, maze, dyna_params):\n",
    "    state = maze.START_STATE\n",
    "    steps = 0\n",
    "    while state not in maze.GOAL_STATES:\n",
    "        # track the steps\n",
    "        steps += 1\n",
    "\n",
    "        # get action\n",
    "        action = choose_action(state, q_value, maze, dyna_params)\n",
    "\n",
    "        # take action\n",
    "        next_state, reward = maze.step(state, action)\n",
    "\n",
    "        # Q-Learning update\n",
    "        q_value[state[0], state[1], action] += \\\n",
    "            dyna_params.alpha * (reward + dyna_params.gamma * np.max(q_value[next_state[0], next_state[1], :]) -\n",
    "                                 q_value[state[0], state[1], action])\n",
    "\n",
    "        # feed the model with experience\n",
    "        model.feed(state, action, next_state, reward)\n",
    "\n",
    "        # sample experience from the model\n",
    "        for t in range(0, dyna_params.planning_steps):\n",
    "            state_, action_, next_state_, reward_ = model.sample()\n",
    "            q_value[state_[0], state_[1], action_] += \\\n",
    "                dyna_params.alpha * (reward_ + dyna_params.gamma * np.max(q_value[next_state_[0], next_state_[1], :]) -\n",
    "                                     q_value[state_[0], state_[1], action_])\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # check whether it has exceeded the step limit\n",
    "        if steps > maze.max_steps:\n",
    "            break\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# play for an episode for prioritized sweeping algorithm\n",
    "# @q_value: state action pair values, will be updated\n",
    "# @model: model instance for planning\n",
    "# @maze: a maze instance containing all information about the environment\n",
    "# @dyna_params: several params for the algorithm\n",
    "# @return: # of backups during this episode\n",
    "def prioritized_sweeping(q_value, model, maze, dyna_params):\n",
    "    state = maze.START_STATE\n",
    "\n",
    "    # track the steps in this episode\n",
    "    steps = 0\n",
    "\n",
    "    # track the backups in planning phase\n",
    "    backups = 0\n",
    "\n",
    "    while state not in maze.GOAL_STATES:\n",
    "        steps += 1\n",
    "\n",
    "        # get action\n",
    "        action = choose_action(state, q_value, maze, dyna_params)\n",
    "\n",
    "        # take action\n",
    "        next_state, reward = maze.step(state, action)\n",
    "\n",
    "        # feed the model with experience\n",
    "        model.feed(state, action, next_state, reward)\n",
    "\n",
    "        # get the priority for current state action pair\n",
    "        priority = np.abs(reward + dyna_params.gamma * np.max(q_value[next_state[0], next_state[1], :]) -\n",
    "                          q_value[state[0], state[1], action])\n",
    "\n",
    "        if priority > dyna_params.theta:\n",
    "            model.insert(priority, state, action)\n",
    "\n",
    "        # start planning\n",
    "        planning_step = 0\n",
    "\n",
    "        # planning for several steps,\n",
    "        # although keep planning until the priority queue becomes empty will converge much faster\n",
    "        while planning_step < dyna_params.planning_steps and not model.empty():\n",
    "            # get a sample with highest priority from the model\n",
    "            priority, state_, action_, next_state_, reward_ = model.sample()\n",
    "\n",
    "            # update the state action value for the sample\n",
    "            delta = reward_ + dyna_params.gamma * np.max(q_value[next_state_[0], next_state_[1], :]) - \\\n",
    "                    q_value[state_[0], state_[1], action_]\n",
    "            q_value[state_[0], state_[1], action_] += dyna_params.alpha * delta\n",
    "\n",
    "            # deal with all the predecessors of the sample state\n",
    "            for state_pre, action_pre, reward_pre in model.predecessor(state_):\n",
    "                priority = np.abs(reward_pre + dyna_params.gamma * np.max(q_value[state_[0], state_[1], :]) -\n",
    "                                  q_value[state_pre[0], state_pre[1], action_pre])\n",
    "                if priority > dyna_params.theta:\n",
    "                    model.insert(priority, state_pre, action_pre)\n",
    "            planning_step += 1\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # update the # of backups\n",
    "        backups += planning_step + 1\n",
    "\n",
    "    return backups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Figure 8.2, DynaMaze, use 10 runs instead of 30 runs\n",
    "def figure_8_2():\n",
    "    # set up an instance for DynaMaze\n",
    "    dyna_maze = Maze()\n",
    "    dyna_params = DynaParams()\n",
    "\n",
    "    runs = 10\n",
    "    episodes = 50\n",
    "    planning_steps = [0, 5, 50]\n",
    "    steps = np.zeros((len(planning_steps), episodes))\n",
    "\n",
    "    for run in tqdm(range(runs)):\n",
    "        for i, planning_step in enumerate(planning_steps):\n",
    "            dyna_params.planning_steps = planning_step\n",
    "            q_value = np.zeros(dyna_maze.q_size)\n",
    "\n",
    "            # generate an instance of Dyna-Q model\n",
    "            model = TrivialModel()\n",
    "            for ep in range(episodes):\n",
    "                # print('run:', run, 'planning step:', planning_step, 'episode:', ep)\n",
    "                steps[i, ep] += dyna_q(q_value, model, dyna_maze, dyna_params)\n",
    "\n",
    "    # averaging over runs\n",
    "    steps /= runs\n",
    "\n",
    "    for i in range(len(planning_steps)):\n",
    "        plt.plot(steps[i, :], label='%d planning steps' % (planning_steps[i]))\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('steps per episode')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# wrapper function for changing maze\n",
    "# @maze: a maze instance\n",
    "# @dynaParams: several parameters for dyna algorithms\n",
    "def changing_maze(maze, dyna_params):\n",
    "\n",
    "    # set up max steps\n",
    "    max_steps = maze.max_steps\n",
    "\n",
    "    # track the cumulative rewards\n",
    "    rewards = np.zeros((dyna_params.runs, 2, max_steps))\n",
    "\n",
    "    for run in tqdm(range(dyna_params.runs)):\n",
    "        # set up models\n",
    "        models = [TrivialModel(), TimeModel(maze, time_weight=dyna_params.time_weight)]\n",
    "\n",
    "        # initialize state action values\n",
    "        q_values = [np.zeros(maze.q_size), np.zeros(maze.q_size)]\n",
    "\n",
    "        for i in range(len(dyna_params.methods)):\n",
    "            # print('run:', run, dyna_params.methods[i])\n",
    "\n",
    "            # set old obstacles for the maze\n",
    "            maze.obstacles = maze.old_obstacles\n",
    "\n",
    "            steps = 0\n",
    "            last_steps = steps\n",
    "            while steps < max_steps:\n",
    "                # play for an episode\n",
    "                steps += dyna_q(q_values[i], models[i], maze, dyna_params)\n",
    "\n",
    "                # update cumulative rewards\n",
    "                rewards[run, i, last_steps: steps] = rewards[run, i, last_steps]\n",
    "                rewards[run, i, min(steps, max_steps - 1)] = rewards[run, i, last_steps] + 1\n",
    "                last_steps = steps\n",
    "\n",
    "                if steps > maze.obstacle_switch_time:\n",
    "                    # change the obstacles\n",
    "                    maze.obstacles = maze.new_obstacles\n",
    "\n",
    "    # averaging over runs\n",
    "    rewards = rewards.mean(axis=0)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Figure 8.4, BlockingMaze\n",
    "def figure_8_4():\n",
    "    # set up a blocking maze instance\n",
    "    blocking_maze = Maze()\n",
    "    blocking_maze.START_STATE = [5, 3]\n",
    "    blocking_maze.GOAL_STATES = [[0, 8]]\n",
    "    blocking_maze.old_obstacles = [[3, i] for i in range(0, 8)]\n",
    "\n",
    "    # new obstalces will block the optimal path\n",
    "    blocking_maze.new_obstacles = [[3, i] for i in range(1, 9)]\n",
    "\n",
    "    # step limit\n",
    "    blocking_maze.max_steps = 3000\n",
    "\n",
    "    # obstacles will change after 1000 steps\n",
    "    # the exact step for changing will be different\n",
    "    # However given that 1000 steps is long enough for both algorithms to converge,\n",
    "    # the difference is guaranteed to be very small\n",
    "    blocking_maze.obstacle_switch_time = 1000\n",
    "\n",
    "    # set up parameters\n",
    "    dyna_params = DynaParams()\n",
    "    dyna_params.alpha = 1.0\n",
    "    dyna_params.planning_steps = 10\n",
    "    dyna_params.runs = 20\n",
    "\n",
    "    # kappa must be small, as the reward for getting the goal is only 1\n",
    "    dyna_params.time_weight = 1e-4\n",
    "\n",
    "    # play\n",
    "    rewards = changing_maze(blocking_maze, dyna_params)\n",
    "\n",
    "    for i in range(len(dyna_params.methods)):\n",
    "        plt.plot(rewards[i, :], label=dyna_params.methods[i])\n",
    "    plt.xlabel('time steps')\n",
    "    plt.ylabel('cumulative reward')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Figure 8.5, ShortcutMaze\n",
    "def figure_8_5():\n",
    "    # set up a shortcut maze instance\n",
    "    shortcut_maze = Maze()\n",
    "    shortcut_maze.START_STATE = [5, 3]\n",
    "    shortcut_maze.GOAL_STATES = [[0, 8]]\n",
    "    shortcut_maze.old_obstacles = [[3, i] for i in range(1, 9)]\n",
    "\n",
    "    # new obstacles will have a shorter path\n",
    "    shortcut_maze.new_obstacles = [[3, i] for i in range(1, 8)]\n",
    "\n",
    "    # step limit\n",
    "    shortcut_maze.max_steps = 6000\n",
    "\n",
    "    # obstacles will change after 3000 steps\n",
    "    # the exact step for changing will be different\n",
    "    # However given that 3000 steps is long enough for both algorithms to converge,\n",
    "    # the difference is guaranteed to be very small\n",
    "    shortcut_maze.obstacle_switch_time = 3000\n",
    "\n",
    "    # set up parameters\n",
    "    dyna_params = DynaParams()\n",
    "\n",
    "    # 50-step planning\n",
    "    dyna_params.planning_steps = 50\n",
    "    dyna_params.runs = 5\n",
    "    dyna_params.time_weight = 1e-3\n",
    "    dyna_params.alpha = 1.0\n",
    "\n",
    "    # play\n",
    "    rewards = changing_maze(shortcut_maze, dyna_params)\n",
    "\n",
    "    for i in range(len(dyna_params.methods)):\n",
    "        plt.plot( rewards[i, :], label=dyna_params.methods[i])\n",
    "    plt.xlabel('time steps')\n",
    "    plt.ylabel('cumulative reward')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Check whether state-action values are already optimal\n",
    "def check_path(q_values, maze):\n",
    "    # get the length of optimal path\n",
    "    # 14 is the length of optimal path of the original maze\n",
    "    # 1.2 means it's a relaxed optifmal path\n",
    "    max_steps = 14 * maze.resolution * 1.2\n",
    "    state = maze.START_STATE\n",
    "    steps = 0\n",
    "    while state not in maze.GOAL_STATES:\n",
    "        action = np.argmax(q_values[state[0], state[1], :])\n",
    "        state, _ = maze.step(state, action)\n",
    "        steps += 1\n",
    "        if steps > max_steps:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Example 8.4, mazes with different resolution\n",
    "def example_8_4():\n",
    "    # get the original 6 * 9 maze\n",
    "    original_maze = Maze()\n",
    "\n",
    "    # set up the parameters for each algorithm\n",
    "    params_dyna = DynaParams()\n",
    "    params_dyna.planning_steps = 5\n",
    "    params_dyna.alpha = 0.5\n",
    "    params_dyna.gamma = 0.95\n",
    "\n",
    "    params_prioritized = DynaParams()\n",
    "    params_prioritized.theta = 0.0001\n",
    "    params_prioritized.planning_steps = 5\n",
    "    params_prioritized.alpha = 0.5\n",
    "    params_prioritized.gamma = 0.95\n",
    "\n",
    "    params = [params_prioritized, params_dyna]\n",
    "\n",
    "    # set up models for planning\n",
    "    models = [PriorityModel, TrivialModel]\n",
    "    method_names = ['Prioritized Sweeping', 'Dyna-Q']\n",
    "\n",
    "    # due to limitation of my machine, I can only perform experiments for 5 mazes\n",
    "    # assuming the 1st maze has w * h states, then k-th maze has w * h * k * k states\n",
    "    num_of_mazes = 5\n",
    "\n",
    "    # build all the mazes\n",
    "    mazes = [original_maze.extend_maze(i) for i in range(1, num_of_mazes + 1)]\n",
    "    methods = [prioritized_sweeping, dyna_q]\n",
    "\n",
    "    # My machine cannot afford too many runs...\n",
    "    runs = 5\n",
    "\n",
    "    # track the # of backups\n",
    "    backups = np.zeros((runs, 2, num_of_mazes))\n",
    "\n",
    "    for run in range(0, runs):\n",
    "        for i in range(0, len(method_names)):\n",
    "            for mazeIndex, maze in zip(range(0, len(mazes)), mazes):\n",
    "                print('run %d, %s, maze size %d' % (run, method_names[i], maze.WORLD_HEIGHT * maze.WORLD_WIDTH))\n",
    "\n",
    "                # initialize the state action values\n",
    "                q_value = np.zeros(maze.q_size)\n",
    "\n",
    "                # track steps / backups for each episode\n",
    "                steps = []\n",
    "\n",
    "                # generate the model\n",
    "                model = models[i]()\n",
    "\n",
    "                # play for an episode\n",
    "                while True:\n",
    "                    steps.append(methods[i](q_value, model, maze, params[i]))\n",
    "\n",
    "                    # print best actions w.r.t. current state-action values\n",
    "                    # printActions(currentStateActionValues, maze)\n",
    "\n",
    "                    # check whether the (relaxed) optimal path is found\n",
    "                    if check_path(q_value, maze):\n",
    "                        break\n",
    "\n",
    "                # update the total steps / backups for this maze\n",
    "                backups[run, i, mazeIndex] = np.sum(steps)\n",
    "\n",
    "    backups = backups.mean(axis=0)\n",
    "\n",
    "    # Dyna-Q performs several backups per step\n",
    "    backups[1, :] *= params_dyna.planning_steps + 1\n",
    "\n",
    "    for i in range(0, len(method_names)):\n",
    "        plt.plot(np.arange(1, num_of_mazes + 1), backups[i, :], label=method_names[i])\n",
    "    plt.xlabel('maze resolution factor')\n",
    "    plt.ylabel('backups until optimal solution')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "figure_8_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "figure_8_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "figure_8_5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_8_4()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
