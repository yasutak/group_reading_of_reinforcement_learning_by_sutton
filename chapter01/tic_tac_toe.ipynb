{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 - 2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)           #\n",
    "# 2016 Jan Hakenberg(jan.hakenberg@gmail.com)                         #\n",
    "# 2016 Tian Jun(tianjun.cpp@gmail.com)                                #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle \n",
    "import pprint as pp\n",
    "\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 3\n",
    "BOARD_SIZE = BOARD_ROWS * BOARD_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self):\n",
    "        # the board is represented by an n * n array, \n",
    "        # 1 represents a chessman of the player who moves first, \n",
    "        # -1 represents a chessman of another player\n",
    "        # 0 represents and empy position\n",
    "        self.data = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.winner = None\n",
    "        self.hash_val = None \n",
    "        self.end = None\n",
    "\n",
    "    # compute the hash value for one state, it's unique\n",
    "    # ref. hash in python https://stackoverflow.com/questions/17585730/what-does-hash-do-in-python\n",
    "    # ref hash function https://en.wikipedia.org/wiki/Hash_function\n",
    "    def hash(self):\n",
    "        if self.hash_val is None:\n",
    "            self.hash_val = 0\n",
    "            for i in np.nditer(self.data):\n",
    "                self.hash_val = self.hash_val * 3 + i + 1\n",
    "        return self.hash_val\n",
    "\n",
    "    # check whether a player has won the game, or it's a tie\n",
    "    def is_end(self):\n",
    "        if self.end is not None:\n",
    "            return self.end\n",
    "        results = []\n",
    "        # check row\n",
    "        for i in range(BOARD_ROWS):\n",
    "            results.append(np.sum(self.data[i, :])) # add numbers in each row\n",
    "        # check columns\n",
    "        for i in range(BOARD_COLS):\n",
    "            results.append(np.sum(self.data[:, i]))\n",
    "\n",
    "        #check diagonals \n",
    "        trace = 0\n",
    "        reverse_trace = 0\n",
    "        for i in range(BOARD_ROWS):\n",
    "            trace += self.data[i, i]\n",
    "            reverse_trace += self.data[i, BOARD_ROWS - 1 - i] #the other diagonal\n",
    "        results.append(trace)\n",
    "        results.append(reverse_trace)\n",
    "\n",
    "        for result in results:\n",
    "            if result == 3:\n",
    "                self.winner = 1\n",
    "                self.end = True\n",
    "                return self.end\n",
    "            if result == -3:\n",
    "                self.winner = -1\n",
    "                self.end = True\n",
    "                return self.end\n",
    "\n",
    "        # whether it's a tie\n",
    "        sum_values = np.sum(np.abs(self.data))\n",
    "        if sum_values == BOARD_SIZE:\n",
    "            self.winner = 0\n",
    "            self.end = True\n",
    "            return self.end\n",
    "        \n",
    "        # game is still going on\n",
    "        self.end = False\n",
    "        return self.end\n",
    "\n",
    "    # @symbol: 1 or -1\n",
    "    # put chessman symbol in position (i, j)\n",
    "    def next_state(self,  i, j, symbol):\n",
    "        new_state = State()\n",
    "        new_state.data = np.copy(self.data)\n",
    "        new_state.data[i, j] = symbol\n",
    "        return new_state\n",
    "\n",
    "    # print the board \n",
    "    def print_state(self):\n",
    "        for i in range(BOARD_ROWS):\n",
    "            print(\"------------\")\n",
    "            out = \"| \"\n",
    "            for j in range(BOARD_COLS):\n",
    "                if self.data[i, j] == 1:\n",
    "                    token = \"*\"\n",
    "                elif self.data[i, j] == -1:\n",
    "                    token = \"x\"\n",
    "                else:\n",
    "                    token = \"0\"\n",
    "                out += token + \" | \"\n",
    "            print(out)\n",
    "        print(\"------------\")\n",
    "\n",
    "# get all configurations of the board recursively\n",
    "def get_all_states_impl(current_state, current_symbol, all_states):\n",
    "    for i in range(BOARD_ROWS):\n",
    "        for j in range(BOARD_COLS):\n",
    "            if current_state.data[i][j] == 0:\n",
    "                new_state = current_state.next_state(i, j, current_symbol)\n",
    "                new_hash = new_state.hash()\n",
    "                if new_hash not in all_states:\n",
    "                    is_end = new_state.is_end()\n",
    "                    all_states[new_hash] = (new_state, is_end)\n",
    "                    if not is_end:\n",
    "                        get_all_states_impl(new_state, -current_symbol, all_states)\n",
    "\n",
    "def get_all_states():\n",
    "    current_symbol = 1\n",
    "    current_state = State()\n",
    "    all_states = dict()\n",
    "    all_states[current_state.hash()] = (current_state, current_state.is_end())\n",
    "    get_all_states_impl(current_state, current_symbol, all_states)\n",
    "    return all_states\n",
    "\n",
    "\n",
    "# all possible board configurations\n",
    "all_states = get_all_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = State()\n",
    "s.print_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_new = s.next_state(0, 0, 1)\n",
    "s_new.print_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "s_new.is_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states[16402]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states[16402][0].print_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states[14764]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states[14764][0].print_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "len(all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# AI player\n",
    "class Player:\n",
    "    # @step_size: the step size to update estimations\n",
    "    # @epsilon: the probability to explore\n",
    "    def __init__(self, step_size=0.1, epsilon=0.1):\n",
    "        self.estimations = dict()\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        self.states = []\n",
    "        self.greedy = []\n",
    "        self.symbol = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.greedy = []\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.states.append(state)\n",
    "        self.greedy.append(True)\n",
    "\n",
    "    #set up initial values\n",
    "    #@init_est: the initial probablity of winning for tie\n",
    "    \"\"\"\n",
    "    Assuming we always play Xs, then for all states with three Xs\n",
    "    in a row the probability of winning is 1, because we have already \n",
    "    won. Similarly, for all states with three Os in a row, or that are filled \n",
    "    up, the correct probability is 0, as we cannot win from them. \n",
    "    We set the initial values of all the other states to 0.5, representing \n",
    "    a guess that we have a 50% chance of winning.(intro to RL by Sutton p9)\n",
    "    \"\"\"\n",
    "    def set_symbol(self, symbol, init_est=0.5):\n",
    "        self.symbol = symbol\n",
    "        for hash_val in all_states:\n",
    "            state, is_end = all_states[hash_val]\n",
    "            if is_end:\n",
    "                if state.winner == self.symbol:\n",
    "                    self.estimations[hash_val] = 1.0\n",
    "                elif state.winner == 0:\n",
    "                    # we need to distinguish between a tie and a lose\n",
    "                    self.estimations[hash_val] = init_est\n",
    "                else:\n",
    "                    self.estimations[hash_val] = 0\n",
    "            else:\n",
    "                self.estimations[hash_val] = 0.5\n",
    "        \n",
    "    # update value estimation\n",
    "    # @exp_included: include exploratory moves in learning\n",
    "    def backup(self, exp_included=False):\n",
    "        states = [state.hash() for state in self.states]\n",
    "\n",
    "        for i in reversed(range(len(states) - 1)):\n",
    "            state = states[i]\n",
    "\n",
    "            if not exp_included:\n",
    "                td_error = self.greedy[i] * ( # greedy consists of True or False(exploratory)\n",
    "                    self.estimations[states[i + 1]] - self.estimations[state]\n",
    "                )\n",
    "            else:\n",
    "                td_error = self.estimations[states[i + 1]] - self.estimations[state]\n",
    "            self.estimations[state] += self.step_size * td_error\n",
    "\n",
    "    # choose an action based on the state\n",
    "    def act(self):\n",
    "        state = self.states[-1]\n",
    "        next_states = []\n",
    "        next_positions = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                if state.data[i, j] == 0:\n",
    "                    next_positions.append([i, j])\n",
    "                    next_states.append(state.next_state(\n",
    "                        i, j, self.symbol).hash())\n",
    "\n",
    "        # expolatory move\n",
    "        \"\"\"\n",
    "        we select randomly from among the other moves instead. \n",
    "        These are called exploratory moves because they cause \n",
    "        us to experience states that we might otherwise never see.\n",
    "        (intro to RL by Sutton p9)\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = next_positions[np.random.randint(len(next_positions))]\n",
    "            action.append(self.symbol)\n",
    "            self.greedy[-1] = False\n",
    "            return action\n",
    "\n",
    "        values = []\n",
    "        for hash_val, pos in zip(next_states, next_positions):\n",
    "            values.append((self.estimations[hash_val], pos))\n",
    "        # to select one of the actions of equal value at random due to Python's sort is stable\n",
    "        # ref. \"Sorts are guaranteed to be stable. That means that when \n",
    "        # multiple records have the same key, their original order is preserved.\"\"\n",
    "        # https://docs.python.org/3/howto/sorting.html#sortinghowto\n",
    "        np.random.shuffle(values)\n",
    "        values.sort(key=lambda x: x[0], reverse=True) # the largest, the first\n",
    "        action = values[0][1]\n",
    "        action.append(self.symbol)\n",
    "        return action\n",
    "\n",
    "    def give_policy(self):\n",
    "        return self.estimations\n",
    "\n",
    "    def save_policy(self):\n",
    "        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'wb') as f:\n",
    "            pickle.dump(self.estimations, f)\n",
    "\n",
    "    def load_policy(self):\n",
    "        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'rb') as f:\n",
    "            self.estimations = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Player(step_size=0.1, epsilon=0.1)\n",
    "p1.estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.set_symbol(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states[15890][0].print_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Judger:\n",
    "    # @player1: the player who will move first, its chessman will be 1\n",
    "    # @player2: another player with a chessman -1\n",
    "    # @init_est: inital estimation of probability of winning for tie\n",
    "    def __init__(self, player1, player2, init_est=0.5):\n",
    "        self.p1 = player1\n",
    "        self.p2 = player2\n",
    "        self.current_player = None \n",
    "        self.p1_symbol = 1\n",
    "        self.p2_symbol = -1\n",
    "        self.p1.set_symbol(self.p1_symbol, init_est)\n",
    "        self.p2.set_symbol(self.p2_symbol, init_est)\n",
    "        self.current_state = State()\n",
    "\n",
    "    def reset(self):\n",
    "        self.p1.reset()\n",
    "        self.p2.reset()\n",
    "\n",
    "    def alternate(self):\n",
    "        while True:\n",
    "            yield self.p1\n",
    "            yield self.p2\n",
    "    \n",
    "    # @print_state: if True, print each board during the game\n",
    "    def play(self, print_state=False):\n",
    "        alternator = self.alternate()\n",
    "        self.reset()\n",
    "        current_state = State()\n",
    "        self.p1.set_state(current_state)\n",
    "        self.p2.set_state(current_state)\n",
    "        if print_state:\n",
    "            current_state.print_state()\n",
    "        while True:\n",
    "            player = next(alternator)\n",
    "            i, j, symbol = player.act()\n",
    "            next_state_hash = current_state.next_state(i, j, symbol).hash()\n",
    "            current_state, is_end = all_states[next_state_hash]\n",
    "            self.p1.set_state(current_state)\n",
    "            self.p2.set_state(current_state)\n",
    "            if print_state:\n",
    "                current_state.print_state()\n",
    "            if is_end:\n",
    "                return current_state.winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "p1 = Player()\n",
    "p2 = Player()\n",
    "judger = Judger(p1, p2)\n",
    "judger.play(print_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# human interface \n",
    "# input a number to pua chessman\n",
    "# | q | w | e |\n",
    "# | a | s | d |\n",
    "# | z | x | c |\n",
    "class HumanPlayer:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.symbol = None\n",
    "        self.keys = ['q', 'w', 'e', 'a', 's', 'd', 'z', 'x', 'c']\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.state = state\n",
    "\n",
    "    def set_symbol(self, symbol, init_est):\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def act(self):\n",
    "        self.state.print_state()\n",
    "        key = input(\"Input your position:\")\n",
    "        data = self.keys.index(key) #keys = ['q', 'w', 'e', ...]\n",
    "        i = data // BOARD_COLS \n",
    "        j = data % BOARD_COLS\n",
    "        return i, j, self.symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# @init_est: the initial probablity of winning for tie\n",
    "# @exp_included: include exploratory moves in learning\n",
    "def train(epochs, print_every_n=500, print_policy=False, init_est=0.5, exp_included=False):\n",
    "    player1 = Player(epsilon=0.01)\n",
    "    player2 = Player(epsilon=0.01)\n",
    "    judger = Judger(player1, player2, init_est)\n",
    "    player1_win = 0.0\n",
    "    player2_win = 0.0\n",
    "    for i in range(1, epochs + 1):\n",
    "        \n",
    "        winner = judger.play(print_state=False)\n",
    "        if winner == 1:\n",
    "            player1_win += 1\n",
    "        if winner == -1:\n",
    "            player2_win += 1\n",
    "        if i % print_every_n == 0:\n",
    "            print('Epoch %d, player 1 winrate: %.02f, player 2 winrate %.02f' % (i, player1_win / i, player2_win / i))\n",
    "        player1.backup()\n",
    "        player2.backup()\n",
    "        judger.reset()\n",
    "    player1.save_policy()\n",
    "    player2.save_policy()\n",
    "    if print_policy:\n",
    "        pp.pprint(player1.estimations)\n",
    "    return (player1.give_policy(), player2.give_policy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "train(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compete(turns):\n",
    "    player1 = Player(epsilon=0)\n",
    "    player2 = Player(epsilon=0)\n",
    "    judger = Judger(player1, player2)\n",
    "    player1.load_policy()\n",
    "    player2.load_policy()\n",
    "    player1_win = 0.0\n",
    "    player2_win = 0.0\n",
    "    for _ in range(turns):\n",
    "        winner = judger.play(print_state=False)\n",
    "        if winner == 1:\n",
    "            player1_win += 1\n",
    "        if winner == -1:\n",
    "            player2_win += 1\n",
    "        judger.reset()\n",
    "    print('%d turns, player 1 win %02f, player 2 win %.02f' % (turns, player1_win / turns, player2_win / turns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "compete(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# The game is a zero sum game. If both players are playing with an optimal strategy, every game will end in a tie. \n",
    "# So we test weather the AI can guarantee at least a tie if it goes second.\n",
    "def play():\n",
    "    while True:\n",
    "        player1 = HumanPlayer()\n",
    "        player2 = Player(epsilon=0)\n",
    "        judger = Judger(player1, player2)\n",
    "        player2.load_policy()\n",
    "        winner = judger.play()\n",
    "        if winner == player2.symbol:\n",
    "            print(\"You lose!\")\n",
    "        elif winner == player1.symbol:\n",
    "            print(\"You win!\")\n",
    "        else:\n",
    "            print(\"It is a tie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(int(1e4))\n",
    "#compete(int(1e3))\n",
    "play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "## Let's change inital estimation of probability of winning for tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# original\n",
    "player1_est_orig = train(int(1e5), init_est=0.5)[0]\n",
    "player1_est_zero = train(int(1e5), init_est=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "est_diff = {}\n",
    "for i in player1_est_orig.keys():\n",
    "    est_diff[i] = player1_est_orig[i] - player1_est_zero[i]\n",
    "\n",
    "pp.pprint(est_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Let's include exploratory moves into learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player1_est_exp = train(int(1e5), init_est=0.5, exp_included=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "est_diff_exp = {}\n",
    "for i in player1_est_orig.keys():\n",
    "    est_diff[i] = player1_est_orig[i] - player1_est_exp[i]\n",
    "\n",
    "pp.pprint(est_diff)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
