{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# 2017 Nicky van Foreest(vanforeest@gmail.com)                        #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# actions: hit or stand\n",
    "ACTION_HIT = 0\n",
    "ACTION_STAND = 1  #  \"strike\" in the book\n",
    "ACTIONS = [ACTION_HIT, ACTION_STAND]\n",
    "\n",
    "# policy for player\n",
    "POLICY_PLAYER = np.zeros(22, dtype=np.int)\n",
    "for i in range(12, 20):\n",
    "    POLICY_PLAYER[i] = ACTION_HIT\n",
    "POLICY_PLAYER[20] = ACTION_STAND\n",
    "POLICY_PLAYER[21] = ACTION_STAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# function form of target policy of player\n",
    "def target_policy_player(usable_ace_player, player_sum, dealer_card):\n",
    "    return POLICY_PLAYER[player_sum]\n",
    "\n",
    "# function form of behavior policy of player\n",
    "def behavior_policy_player(usable_ace_player, player_sum, dealer_card):\n",
    "    if np.random.binomial(1, 0.5) == 1:\n",
    "        return ACTION_STAND\n",
    "    return ACTION_HIT\n",
    "\n",
    "# policy for dealer\n",
    "POLICY_DEALER = np.zeros(22)\n",
    "for i in range(12, 17):\n",
    "    POLICY_DEALER[i] = ACTION_HIT\n",
    "for i in range(17, 22):\n",
    "    POLICY_DEALER[i] = ACTION_STAND\n",
    "\n",
    "# get a new card\n",
    "def get_card():\n",
    "    card = np.random.randint(1, 14)\n",
    "    card = min(card, 10)\n",
    "    return card\n",
    "\n",
    "# get the value of a card (11 for ace).\n",
    "def card_value(card_id):\n",
    "    return 11 if card_id == 1 else card_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# play a game\n",
    "# @policy_player: specify policy for player\n",
    "# @initial_state: [whether player has a usable Ace, sum of player's cards, one card of dealer]\n",
    "# @initial_action: the initial action\n",
    "def play(policy_player, initial_state=None, initial_action=None):\n",
    "    # player status\n",
    "\n",
    "    # sum of player\n",
    "    player_sum = 0\n",
    "\n",
    "    # trajectory of player\n",
    "    player_trajectory = []\n",
    "\n",
    "    # whether player uses Ace as 11\n",
    "    usable_ace_player = False\n",
    "\n",
    "    # dealer status\n",
    "    dealer_card1 = 0\n",
    "    dealer_card2 = 0\n",
    "    usable_ace_dealer = False\n",
    "\n",
    "    if initial_state is None:\n",
    "        # generate a random initial state\n",
    "\n",
    "        while player_sum < 12:\n",
    "            # if sum of player is less than 12, always hit\n",
    "            card = get_card()\n",
    "            player_sum += card_value(card)\n",
    "\n",
    "            # If the player's sum is larger than 21, he may hold one or two aces.\n",
    "            if player_sum > 21:\n",
    "                assert player_sum == 22\n",
    "                # last card must be ace\n",
    "                player_sum -= 10\n",
    "            else:\n",
    "                usable_ace_player |= (1 == card)\n",
    "\n",
    "        # initialize cards of dealer, suppose dealer will show the first card he gets\n",
    "        dealer_card1 = get_card()\n",
    "        dealer_card2 = get_card()\n",
    "\n",
    "    else:\n",
    "        # use specified initial state\n",
    "        usable_ace_player, player_sum, dealer_card1 = initial_state\n",
    "        dealer_card2 = get_card()\n",
    "\n",
    "    # initial state of the game\n",
    "    state = [usable_ace_player, player_sum, dealer_card1]\n",
    "\n",
    "    # initialize dealer's sum\n",
    "    dealer_sum = card_value(dealer_card1) + card_value(dealer_card2)\n",
    "    usable_ace_dealer = 1 in (dealer_card1, dealer_card2)\n",
    "    # if the dealer's sum is larger than 21, he must hold two aces.\n",
    "    if dealer_sum > 21:\n",
    "        assert dealer_sum == 22\n",
    "        # use one Ace as 1 rather than 11\n",
    "        dealer_sum -= 10\n",
    "    assert dealer_sum <= 21\n",
    "    assert player_sum <= 21\n",
    "\n",
    "    # game starts!\n",
    "\n",
    "    # player's turn\n",
    "    while True:\n",
    "        if initial_action is not None:\n",
    "            action = initial_action\n",
    "            initial_action = None\n",
    "        else:\n",
    "            # get action based on current sum\n",
    "            action = policy_player(usable_ace_player, player_sum, dealer_card1)\n",
    "\n",
    "        # track player's trajectory for importance sampling\n",
    "        player_trajectory.append([(usable_ace_player, player_sum, dealer_card1), action])\n",
    "\n",
    "        if action == ACTION_STAND:\n",
    "            break\n",
    "        # if hit, get new card\n",
    "        card = get_card()\n",
    "        # Keep track of the ace count. the usable_ace_player flag is insufficient alone as it cannot\n",
    "        # distinguish between having one ace or two.\n",
    "        ace_count = int(usable_ace_player)\n",
    "        if card == 1:\n",
    "            ace_count += 1\n",
    "        player_sum += card_value(card)\n",
    "        # If the player has a usable ace, use it as 1 to avoid busting and continue.\n",
    "        while player_sum > 21 and ace_count:\n",
    "            player_sum -= 10\n",
    "            ace_count -= 1\n",
    "        # player busts\n",
    "        if player_sum > 21:\n",
    "            return state, -1, player_trajectory\n",
    "        assert player_sum <= 21\n",
    "        usable_ace_player = (ace_count == 1)\n",
    "\n",
    "    # dealer's turn\n",
    "    while True:\n",
    "        # get action based on current sum\n",
    "        action = POLICY_DEALER[dealer_sum]\n",
    "        if action == ACTION_STAND:\n",
    "            break\n",
    "        # if hit, get a new card\n",
    "        new_card = get_card()\n",
    "        ace_count = int(usable_ace_dealer)\n",
    "        if new_card == 1:\n",
    "            ace_count += 1\n",
    "        dealer_sum += card_value(new_card)\n",
    "        # If the dealer has a usable ace, use it as 1 to avoid busting and continue.\n",
    "        while dealer_sum > 21 and ace_count:\n",
    "            dealer_sum -= 10\n",
    "            ace_count -= 1\n",
    "        # dealer busts\n",
    "        if dealer_sum > 21:\n",
    "            return state, 1, player_trajectory\n",
    "        usable_ace_dealer = (ace_count == 1)\n",
    "\n",
    "    # compare the sum between player and dealer\n",
    "    assert player_sum <= 21 and dealer_sum <= 21\n",
    "    if player_sum > dealer_sum:\n",
    "        return state, 1, player_trajectory\n",
    "    elif player_sum == dealer_sum:\n",
    "        return state, 0, player_trajectory\n",
    "    else:\n",
    "        return state, -1, player_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Monte Carlo Sample with On-Policy\n",
    "def monte_carlo_on_policy(episodes):\n",
    "    states_usable_ace = np.zeros((10, 10))\n",
    "    # initialze counts to 1 to avoid 0 being divided\n",
    "    states_usable_ace_count = np.ones((10, 10))\n",
    "    states_no_usable_ace = np.zeros((10, 10))\n",
    "    # initialze counts to 1 to avoid 0 being divided\n",
    "    states_no_usable_ace_count = np.ones((10, 10))\n",
    "    for i in tqdm(range(0, episodes)):\n",
    "        _, reward, player_trajectory = play(target_policy_player)\n",
    "        for (usable_ace, player_sum, dealer_card), _ in player_trajectory:\n",
    "            player_sum -= 12\n",
    "            dealer_card -= 1\n",
    "            if usable_ace:\n",
    "                states_usable_ace_count[player_sum, dealer_card] += 1\n",
    "                states_usable_ace[player_sum, dealer_card] += reward\n",
    "            else:\n",
    "                states_no_usable_ace_count[player_sum, dealer_card] += 1\n",
    "                states_no_usable_ace[player_sum, dealer_card] += reward\n",
    "    return states_usable_ace / states_usable_ace_count, states_no_usable_ace / states_no_usable_ace_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Monte Carlo with Exploring Starts\n",
    "def monte_carlo_es(episodes):\n",
    "    # (playerSum, dealerCard, usableAce, action)\n",
    "    state_action_values = np.zeros((10, 10, 2, 2))\n",
    "    # initialze counts to 1 to avoid division by 0\n",
    "    state_action_pair_count = np.ones((10, 10, 2, 2))\n",
    "\n",
    "    # behavior policy is greedy\n",
    "    def behavior_policy(usable_ace, player_sum, dealer_card):\n",
    "        usable_ace = int(usable_ace)\n",
    "        player_sum -= 12\n",
    "        dealer_card -= 1\n",
    "        # get argmax of the average returns(s, a)\n",
    "        values_ = state_action_values[player_sum, dealer_card, usable_ace, :] / \\\n",
    "                  state_action_pair_count[player_sum, dealer_card, usable_ace, :]\n",
    "        return np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])\n",
    "\n",
    "    # play for several episodes\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        # for each episode, use a randomly initialized state and action\n",
    "        initial_state = [bool(np.random.choice([0, 1])),\n",
    "                       np.random.choice(range(12, 22)),\n",
    "                       np.random.choice(range(1, 11))]\n",
    "        initial_action = np.random.choice(ACTIONS)\n",
    "        current_policy = behavior_policy if episode else target_policy_player\n",
    "        _, reward, trajectory = play(current_policy, initial_state, initial_action)\n",
    "        first_visit_check = set()\n",
    "        for (usable_ace, player_sum, dealer_card), action in trajectory:\n",
    "            usable_ace = int(usable_ace)\n",
    "            player_sum -= 12\n",
    "            dealer_card -= 1\n",
    "            state_action = (usable_ace, player_sum, dealer_card, action)\n",
    "            if state_action in first_visit_check:\n",
    "                continue\n",
    "            first_visit_check.add(state_action)\n",
    "            # update values of state-action pairs\n",
    "            state_action_values[player_sum, dealer_card, usable_ace, action] += reward\n",
    "            state_action_pair_count[player_sum, dealer_card, usable_ace, action] += 1\n",
    "\n",
    "    return state_action_values / state_action_pair_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Monte Carlo Sample with Off-Policy\n",
    "def monte_carlo_off_policy(episodes):\n",
    "    initial_state = [True, 13, 2]\n",
    "\n",
    "    rhos = []\n",
    "    returns = []\n",
    "\n",
    "    for i in range(0, episodes):\n",
    "        _, reward, player_trajectory = play(behavior_policy_player, initial_state=initial_state)\n",
    "\n",
    "        # get the importance ratio\n",
    "        numerator = 1.0\n",
    "        denominator = 1.0\n",
    "        for (usable_ace, player_sum, dealer_card), action in player_trajectory:\n",
    "            if action == target_policy_player(usable_ace, player_sum, dealer_card):\n",
    "                denominator *= 0.5\n",
    "            else:\n",
    "                numerator = 0.0\n",
    "                break\n",
    "        rho = numerator / denominator\n",
    "        rhos.append(rho)\n",
    "        returns.append(reward)\n",
    "\n",
    "    rhos = np.asarray(rhos)\n",
    "    returns = np.asarray(returns)\n",
    "    weighted_returns = rhos * returns\n",
    "\n",
    "    weighted_returns = np.add.accumulate(weighted_returns)\n",
    "    rhos = np.add.accumulate(rhos)\n",
    "\n",
    "    ordinary_sampling = weighted_returns / np.arange(1, episodes + 1)\n",
    "\n",
    "    with np.errstate(divide='ignore',invalid='ignore'):\n",
    "        weighted_sampling = np.where(rhos != 0, weighted_returns / rhos, 0)\n",
    "\n",
    "    return ordinary_sampling, weighted_sampling\n",
    "\n",
    "def figure_5_1():\n",
    "    states_usable_ace_1, states_no_usable_ace_1 = monte_carlo_on_policy(10000)\n",
    "    states_usable_ace_2, states_no_usable_ace_2 = monte_carlo_on_policy(500000)\n",
    "\n",
    "    states = [states_usable_ace_1,\n",
    "              states_usable_ace_2,\n",
    "              states_no_usable_ace_1,\n",
    "              states_no_usable_ace_2]\n",
    "\n",
    "    titles = ['Usable Ace, 10000 Episodes',\n",
    "              'Usable Ace, 500000 Episodes',\n",
    "              'No Usable Ace, 10000 Episodes',\n",
    "              'No Usable Ace, 500000 Episodes']\n",
    "\n",
    "    _, axes = plt.subplots(2, 2, figsize=(40, 30))\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for state, title, axis in zip(states, titles, axes):\n",
    "        fig = sns.heatmap(np.flipud(state), cmap=\"YlGnBu\", ax=axis, xticklabels=range(1, 11),\n",
    "                          yticklabels=list(reversed(range(12, 22))))\n",
    "        fig.set_ylabel('player sum', fontsize=30)\n",
    "        fig.set_xlabel('dealer showing', fontsize=30)\n",
    "        fig.set_title(title, fontsize=30)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def figure_5_2():\n",
    "    state_action_values = monte_carlo_es(500000)\n",
    "\n",
    "    state_value_no_usable_ace = np.max(state_action_values[:, :, 0, :], axis=-1)\n",
    "    state_value_usable_ace = np.max(state_action_values[:, :, 1, :], axis=-1)\n",
    "\n",
    "    # get the optimal policy\n",
    "    action_no_usable_ace = np.argmax(state_action_values[:, :, 0, :], axis=-1)\n",
    "    action_usable_ace = np.argmax(state_action_values[:, :, 1, :], axis=-1)\n",
    "\n",
    "    images = [action_usable_ace,\n",
    "              state_value_usable_ace,\n",
    "              action_no_usable_ace,\n",
    "              state_value_no_usable_ace]\n",
    "\n",
    "    titles = ['Optimal policy with usable Ace',\n",
    "              'Optimal value with usable Ace',\n",
    "              'Optimal policy without usable Ace',\n",
    "              'Optimal value without usable Ace']\n",
    "\n",
    "    _, axes = plt.subplots(2, 2, figsize=(40, 30))\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for image, title, axis in zip(images, titles, axes):\n",
    "        fig = sns.heatmap(np.flipud(image), cmap=\"YlGnBu\", ax=axis, xticklabels=range(1, 11),\n",
    "                          yticklabels=list(reversed(range(12, 22))))\n",
    "        fig.set_ylabel('player sum', fontsize=30)\n",
    "        fig.set_xlabel('dealer showing', fontsize=30)\n",
    "        fig.set_title(title, fontsize=30)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def figure_5_3():\n",
    "    true_value = -0.27726\n",
    "    episodes = 10000\n",
    "    runs = 100\n",
    "    error_ordinary = np.zeros(episodes)\n",
    "    error_weighted = np.zeros(episodes)\n",
    "    for i in tqdm(range(0, runs)):\n",
    "        ordinary_sampling_, weighted_sampling_ = monte_carlo_off_policy(episodes)\n",
    "        # get the squared error\n",
    "        error_ordinary += np.power(ordinary_sampling_ - true_value, 2)\n",
    "        error_weighted += np.power(weighted_sampling_ - true_value, 2)\n",
    "    error_ordinary /= runs\n",
    "    error_weighted /= runs\n",
    "\n",
    "    plt.plot(error_ordinary, label='Ordinary Importance Sampling')\n",
    "    plt.plot(error_weighted, label='Weighted Importance Sampling')\n",
    "    plt.xlabel('Episodes (log scale)')\n",
    "    plt.ylabel('Mean square error')\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "figure_5_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "figure_5_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "figure_5_3()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
