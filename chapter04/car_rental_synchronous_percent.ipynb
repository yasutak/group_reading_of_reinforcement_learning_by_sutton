{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# 2017 Aja Rangaswamy (aja004@gmail.com)                              #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "# This file is contributed by Tahsincan KÃ¶se which implements a synchronous policy evaluation, while the car_rental.py\n",
    "# implements an asynchronous policy evaluation. This file also utilizes multi-processing for acceleration and contains\n",
    "# an answer to Exercise 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tqdm\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "############# PROBLEM SPECIFIC CONSTANTS #######################\n",
    "MAX_CARS = 20\n",
    "MAX_MOVE = 5\n",
    "MOVE_COST = -2\n",
    "ADDITIONAL_PARK_COST = -4\n",
    "\n",
    "RENT_REWARD = 10\n",
    "# expectation for rental requests in first location\n",
    "RENTAL_REQUEST_FIRST_LOC = 3\n",
    "# expectation for rental requests in second location\n",
    "RENTAL_REQUEST_SECOND_LOC = 4\n",
    "# expectation for # of cars returned in first location\n",
    "RETURNS_FIRST_LOC = 3\n",
    "# expectation for # of cars returned in second location\n",
    "RETURNS_SECOND_LOC = 2\n",
    "################################################################\n",
    "\n",
    "poisson_cache = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def poisson(n, lam):\n",
    "    global poisson_cache\n",
    "    key = n * 10 + lam\n",
    "    if key not in poisson_cache.keys():\n",
    "        poisson_cache[key] = math.exp(-lam) * math.pow(lam, n) / math.factorial(n)\n",
    "    return poisson_cache[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(self, truncate, parallel_processes, delta=1e-2, gamma=0.9, solve_4_5=False):\n",
    "        self.TRUNCATE = truncate\n",
    "        self.NR_PARALLEL_PROCESSES = parallel_processes\n",
    "        self.actions = np.arange(-MAX_MOVE, MAX_MOVE + 1)\n",
    "        self.inverse_actions = {el: ind[0] for ind, el in np.ndenumerate(self.actions)}\n",
    "        self.values = np.zeros((MAX_CARS + 1, MAX_CARS + 1))\n",
    "        self.policy = np.zeros(self.values.shape, dtype=np.int)\n",
    "        self.delta = delta\n",
    "        self.gamma = gamma\n",
    "        self.solve_extension = solve_4_5\n",
    "\n",
    "    def solve(self):\n",
    "        iterations = 0\n",
    "        total_start_time = time.time()\n",
    "        while True:\n",
    "            start_time = time.time()\n",
    "            self.values = self.policy_evaluation(self.values, self.policy)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f'PE => Elapsed time {elapsed_time} seconds')\n",
    "            start_time = time.time()\n",
    "\n",
    "            policy_change, self.policy = self.policy_improvement(self.actions, self.values, self.policy)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f'PI => Elapsed time {elapsed_time} seconds')\n",
    "            if policy_change == 0:\n",
    "                break\n",
    "            iterations += 1\n",
    "        total_elapsed_time = time.time() - total_start_time\n",
    "        print(f'Optimal policy is reached after {iterations} iterations in {total_elapsed_time} seconds')\n",
    "\n",
    "    # out-place\n",
    "    def policy_evaluation(self, values, policy):\n",
    "\n",
    "        global MAX_CARS\n",
    "        while True:\n",
    "            new_values = np.copy(values)\n",
    "            k = np.arange(MAX_CARS + 1)\n",
    "            # cartesian product\n",
    "            all_states = ((i, j) for i, j in itertools.product(k, k))\n",
    "\n",
    "            results = []\n",
    "            with mp.Pool(processes=self.NR_PARALLEL_PROCESSES) as p:\n",
    "                cook = partial(self.expected_return_pe, policy, values)\n",
    "                results = p.map(cook, all_states)\n",
    "\n",
    "            for v, i, j in results:\n",
    "                new_values[i, j] = v\n",
    "\n",
    "            difference = np.abs(new_values - values).sum()\n",
    "            print(f'Difference: {difference}')\n",
    "            values = new_values\n",
    "            if difference < self.delta:\n",
    "                print(f'Values are converged!')\n",
    "                return values\n",
    "\n",
    "    def policy_improvement(self, actions, values, policy):\n",
    "        new_policy = np.copy(policy)\n",
    "\n",
    "        expected_action_returns = np.zeros((MAX_CARS + 1, MAX_CARS + 1, np.size(actions)))\n",
    "        cooks = dict()\n",
    "        with mp.Pool(processes=8) as p:\n",
    "            for action in actions:\n",
    "                k = np.arange(MAX_CARS + 1)\n",
    "                all_states = ((i, j) for i, j in itertools.product(k, k))\n",
    "                cooks[action] = partial(self.expected_return_pi, values, action)\n",
    "                results = p.map(cooks[action], all_states)\n",
    "                for v, i, j, a in results:\n",
    "                    expected_action_returns[i, j, self.inverse_actions[a]] = v\n",
    "        for i in range(expected_action_returns.shape[0]):\n",
    "            for j in range(expected_action_returns.shape[1]):\n",
    "                new_policy[i, j] = actions[np.argmax(expected_action_returns[i, j])]\n",
    "\n",
    "        policy_change = (new_policy != policy).sum()\n",
    "        print(f'Policy changed in {policy_change} states')\n",
    "        return policy_change, new_policy\n",
    "\n",
    "    # O(n^4) computation for all possible requests and returns\n",
    "    def bellman(self, values, action, state):\n",
    "        expected_return = 0\n",
    "        if self.solve_extension:\n",
    "            if action > 0:\n",
    "                # Free shuttle to the second location\n",
    "                expected_return += MOVE_COST * (action - 1)\n",
    "            else:\n",
    "                expected_return += MOVE_COST * abs(action)\n",
    "        else:\n",
    "            expected_return += MOVE_COST * abs(action)\n",
    "\n",
    "        for req1 in range(0, self.TRUNCATE):\n",
    "            for req2 in range(0, self.TRUNCATE):\n",
    "                # moving cars\n",
    "                num_of_cars_first_loc = int(min(state[0] - action, MAX_CARS))\n",
    "                num_of_cars_second_loc = int(min(state[1] + action, MAX_CARS))\n",
    "\n",
    "                # valid rental requests should be less than actual # of cars\n",
    "                real_rental_first_loc = min(num_of_cars_first_loc, req1)\n",
    "                real_rental_second_loc = min(num_of_cars_second_loc, req2)\n",
    "\n",
    "                # get credits for renting\n",
    "                reward = (real_rental_first_loc + real_rental_second_loc) * RENT_REWARD\n",
    "\n",
    "                if self.solve_extension:\n",
    "                    if num_of_cars_first_loc >= 10:\n",
    "                        reward += ADDITIONAL_PARK_COST\n",
    "                    if num_of_cars_second_loc >= 10:\n",
    "                        reward += ADDITIONAL_PARK_COST\n",
    "\n",
    "                num_of_cars_first_loc -= real_rental_first_loc\n",
    "                num_of_cars_second_loc -= real_rental_second_loc\n",
    "\n",
    "                # probability for current combination of rental requests\n",
    "                prob = poisson(req1, RENTAL_REQUEST_FIRST_LOC) * \\\n",
    "                       poisson(req2, RENTAL_REQUEST_SECOND_LOC)\n",
    "                for ret1 in range(0, self.TRUNCATE):\n",
    "                    for ret2 in range(0, self.TRUNCATE):\n",
    "                        num_of_cars_first_loc_ = min(num_of_cars_first_loc + ret1, MAX_CARS)\n",
    "                        num_of_cars_second_loc_ = min(num_of_cars_second_loc + ret2, MAX_CARS)\n",
    "                        prob_ = poisson(ret1, RETURNS_FIRST_LOC) * \\\n",
    "                                poisson(ret2, RETURNS_SECOND_LOC) * prob\n",
    "                        # Classic Bellman equation for state-value\n",
    "                        # prob_ corresponds to p(s'|s,a) for each possible s' -> (num_of_cars_first_loc_,num_of_cars_second_loc_)\n",
    "                        expected_return += prob_ * (\n",
    "                                reward + self.gamma * values[num_of_cars_first_loc_, num_of_cars_second_loc_])\n",
    "        return expected_return\n",
    "\n",
    "    # Parallelization enforced different helper functions\n",
    "    # Expected return calculator for Policy Evaluation\n",
    "    def expected_return_pe(self, policy, values, state):\n",
    "\n",
    "        action = policy[state[0], state[1]]\n",
    "        expected_return = self.bellman(values, action, state)\n",
    "        return expected_return, state[0], state[1]\n",
    "\n",
    "    # Expected return calculator for Policy Improvement\n",
    "    def expected_return_pi(self, values, action, state):\n",
    "\n",
    "        if ((action >= 0 and state[0] >= action) or (action < 0 and state[1] >= abs(action))) == False:\n",
    "            return -float('inf'), state[0], state[1], action\n",
    "        expected_return = self.bellman(values, action, state)\n",
    "        return expected_return, state[0], state[1], action\n",
    "\n",
    "    def plot(self):\n",
    "        print(self.policy)\n",
    "        plt.figure()\n",
    "        plt.xlim(0, MAX_CARS + 1)\n",
    "        plt.ylim(0, MAX_CARS + 1)\n",
    "        plt.table(cellText=np.flipud(self.policy), loc=(0, 0), cellLoc='center')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "TRUNCATE = 9\n",
    "solver = PolicyIteration(TRUNCATE, parallel_processes=4, delta=1e-1, gamma=0.9, solve_4_5=True)\n",
    "solver.solve()\n",
    "solver.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
