{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.table import Table\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#matplotlib.use('Agg')\n",
    "\n",
    "WORLD_SIZE = 4\n",
    "# left, up, right, down\n",
    "ACTIONS = [np.array([0, -1]),\n",
    "           np.array([-1, 0]),\n",
    "           np.array([0, 1]),\n",
    "           np.array([1, 0])]\n",
    "ACTION_PROB = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def is_terminal(state):\n",
    "    x, y = state\n",
    "    return (x == 0 and y == 0) or (x == WORLD_SIZE - 1 and y == WORLD_SIZE - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    if is_terminal(state):\n",
    "        return state, 0\n",
    "\n",
    "    next_state = (np.array(state) + action).tolist()\n",
    "    x, y = next_state\n",
    "\n",
    "    if x < 0 or x >= WORLD_SIZE or y < 0 or y >= WORLD_SIZE:\n",
    "        next_state = state\n",
    "\n",
    "    reward = -1\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def draw_image(image):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    tb = Table(ax, bbox=[0, 0, 1, 1])\n",
    "\n",
    "    nrows, ncols = image.shape\n",
    "    width, height = 1.0 / ncols, 1.0 / nrows\n",
    "\n",
    "    # Add cells\n",
    "    for (i, j), val in np.ndenumerate(image):\n",
    "        tb.add_cell(i, j, width, height, text=val,\n",
    "                    loc='center', facecolor='white')\n",
    "\n",
    "        # Row and column labels...\n",
    "    for i in range(len(image)):\n",
    "        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n",
    "                    edgecolor='none', facecolor='none')\n",
    "        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n",
    "                    edgecolor='none', facecolor='none')\n",
    "    ax.add_table(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_state_value(in_place=True, discount=1.0):\n",
    "    new_state_values = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        if in_place:\n",
    "            state_values = new_state_values\n",
    "        else:\n",
    "            state_values = new_state_values.copy()\n",
    "        old_state_values = state_values.copy()\n",
    "\n",
    "        for i in range(WORLD_SIZE):\n",
    "            for j in range(WORLD_SIZE):\n",
    "                value = 0\n",
    "                for action in ACTIONS:\n",
    "                    (next_i, next_j), reward = step([i, j], action)\n",
    "                    value += ACTION_PROB * (reward + discount * state_values[next_i, next_j])\n",
    "                new_state_values[i, j] = value\n",
    "\n",
    "        max_delta_value = abs(old_state_values - new_state_values).max()\n",
    "        if max_delta_value < 1e-4:\n",
    "            break\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return new_state_values, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_4_1():\n",
    "    # While the author suggests using in-place iterative policy evaluation,\n",
    "    # Figure 4.1 actually uses out-of-place version.\n",
    "    _, asycn_iteration = compute_state_value(in_place=True)\n",
    "    values, sync_iteration = compute_state_value(in_place=False)\n",
    "    draw_image(np.round(values, decimals=2))\n",
    "    print('In-place: {} iterations'.format(asycn_iteration))\n",
    "    print('Synchronous: {} iterations'.format(sync_iteration))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_4_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
