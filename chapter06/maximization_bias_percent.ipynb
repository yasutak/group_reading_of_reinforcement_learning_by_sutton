{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# state A\n",
    "STATE_A = 0\n",
    "\n",
    "# state B\n",
    "STATE_B = 1\n",
    "\n",
    "# use one terminal state\n",
    "STATE_TERMINAL = 2\n",
    "\n",
    "# starts from state A\n",
    "STATE_START = STATE_A\n",
    "\n",
    "# possible actions in A\n",
    "ACTION_A_RIGHT = 0\n",
    "ACTION_A_LEFT = 1\n",
    "\n",
    "# probability for exploration\n",
    "EPSILON = 0.1\n",
    "\n",
    "# step size\n",
    "ALPHA = 0.1\n",
    "\n",
    "# discount for max value\n",
    "GAMMA = 1.0\n",
    "\n",
    "# possible actions in B, maybe 10 actions\n",
    "ACTIONS_B = range(0, 10)\n",
    "\n",
    "# all possible actions\n",
    "STATE_ACTIONS = [[ACTION_A_RIGHT, ACTION_A_LEFT], ACTIONS_B]\n",
    "\n",
    "# state action pair values, if a state is a terminal state, then the value is always 0\n",
    "INITIAL_Q = [np.zeros(2), np.zeros(len(ACTIONS_B)), np.zeros(1)]\n",
    "\n",
    "# set up destination for each state and each action\n",
    "TRANSITION = [[STATE_TERMINAL, STATE_B], [STATE_TERMINAL] * len(ACTIONS_B)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# choose an action based on epsilon greedy algorithm\n",
    "def choose_action(state, q_value):\n",
    "    if np.random.binomial(1, EPSILON) == 1:\n",
    "        return np.random.choice(STATE_ACTIONS[state])\n",
    "    else:\n",
    "        values_ = q_value[state]\n",
    "        return np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# take @action in @state, return the reward\n",
    "def take_action(state, action):\n",
    "    if state == STATE_A:\n",
    "        return 0\n",
    "    return np.random.normal(-0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# if there are two state action pair value array, use double Q-Learning\n",
    "# otherwise use normal Q-Learning\n",
    "def q_learning(q1, q2=None):\n",
    "    state = STATE_START\n",
    "    # track the # of action left in state A\n",
    "    left_count = 0\n",
    "    while state != STATE_TERMINAL:\n",
    "        if q2 is None:\n",
    "            action = choose_action(state, q1)\n",
    "        else:\n",
    "            # derive a action form Q1 and Q2\n",
    "            action = choose_action(state, [item1 + item2 for item1, item2 in zip(q1, q2)])\n",
    "        if state == STATE_A and action == ACTION_A_LEFT:\n",
    "            left_count += 1\n",
    "        reward = take_action(state, action)\n",
    "        next_state = TRANSITION[state][action]\n",
    "        if q2 is None:\n",
    "            active_q = q1\n",
    "            target = np.max(active_q[next_state])\n",
    "        else:\n",
    "            if np.random.binomial(1, 0.5) == 1:\n",
    "                active_q = q1\n",
    "                target_q = q2\n",
    "            else:\n",
    "                active_q = q2\n",
    "                target_q = q1\n",
    "            best_action = np.random.choice([action_ for action_, value_ in enumerate(active_q[next_state]) if value_ == np.max(active_q[next_state])])\n",
    "            target = target_q[next_state][best_action]\n",
    "\n",
    "        # Q-Learning update\n",
    "        active_q[state][action] += ALPHA * (\n",
    "            reward + GAMMA * target - active_q[state][action])\n",
    "        state = next_state\n",
    "    return left_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 6.7, 1,000 runs may be enough, # of actions in state B will also affect the curves\n",
    "def figure_6_7():\n",
    "    # each independent run has 300 episodes\n",
    "    episodes = 300\n",
    "    runs = 1000\n",
    "    left_counts_q = np.zeros((runs, episodes))\n",
    "    left_counts_double_q = np.zeros((runs, episodes))\n",
    "    for run in tqdm(range(runs)):\n",
    "        q = copy.deepcopy(INITIAL_Q)\n",
    "        q1 = copy.deepcopy(INITIAL_Q)\n",
    "        q2 = copy.deepcopy(INITIAL_Q)\n",
    "        for ep in range(0, episodes):\n",
    "            left_counts_q[run, ep] = q_learning(q)\n",
    "            left_counts_double_q[run, ep] = q_learning(q1, q2)\n",
    "    left_counts_q = left_counts_q.mean(axis=0)\n",
    "    left_counts_double_q = left_counts_double_q.mean(axis=0)\n",
    "\n",
    "    plt.plot(left_counts_q, label='Q-Learning')\n",
    "    plt.plot(left_counts_double_q, label='Double Q-Learning')\n",
    "    plt.plot(np.ones(episodes) * 0.05, label='Optimal')\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('% left actions from A')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "figure_6_7()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
