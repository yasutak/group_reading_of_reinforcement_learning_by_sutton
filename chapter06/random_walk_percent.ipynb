{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 0 is the left terminal state\n",
    "# 6 is the right terminal state\n",
    "# 1 ... 5 represents A ... E\n",
    "VALUES = np.zeros(7)\n",
    "VALUES[1:6] = 0.5\n",
    "# For convenience, we assume all rewards are 0\n",
    "# and the left terminal state has value 0, the right terminal state has value 1\n",
    "# This trick has been used in Gambler's Problem\n",
    "VALUES[6] = 1\n",
    "\n",
    "# set up true state values\n",
    "TRUE_VALUE = np.zeros(7)\n",
    "TRUE_VALUE[1:6] = np.arange(1, 6) / 6.0\n",
    "TRUE_VALUE[6] = 1\n",
    "\n",
    "ACTION_LEFT = 0\n",
    "ACTION_RIGHT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# @values: current states value, will be updated if @batch is False\n",
    "# @alpha: step size\n",
    "# @batch: whether to update @values\n",
    "def temporal_difference(values, alpha=0.1, batch=False):\n",
    "    state = 3\n",
    "    trajectory = [state]\n",
    "    rewards = [0]\n",
    "    while True:\n",
    "        old_state = state\n",
    "        if np.random.binomial(1, 0.5) == ACTION_LEFT:\n",
    "            state -= 1\n",
    "        else:\n",
    "            state += 1\n",
    "        # Assume all rewards are 0\n",
    "        reward = 0\n",
    "        trajectory.append(state)\n",
    "        # TD update\n",
    "        if not batch:\n",
    "            values[old_state] += alpha * (reward + values[state] - values[old_state])\n",
    "        if state == 6 or state == 0:\n",
    "            break\n",
    "        rewards.append(reward)\n",
    "    return trajectory, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# @values: current states value, will be updated if @batch is False\n",
    "# @alpha: step size\n",
    "# @batch: whether to update @values\n",
    "def monte_carlo(values, alpha=0.1, batch=False):\n",
    "    state = 3\n",
    "    trajectory = [3]\n",
    "\n",
    "    # if end up with left terminal state, all returns are 0\n",
    "    # if end up with right terminal state, all returns are 1\n",
    "    while True:\n",
    "        if np.random.binomial(1, 0.5) == ACTION_LEFT:\n",
    "            state -= 1\n",
    "        else:\n",
    "            state += 1\n",
    "        trajectory.append(state)\n",
    "        if state == 6:\n",
    "            returns = 1.0\n",
    "            break\n",
    "        elif state == 0:\n",
    "            returns = 0.0\n",
    "            break\n",
    "\n",
    "    if not batch:\n",
    "        for state_ in trajectory[:-1]:\n",
    "            # MC update\n",
    "            values[state_] += alpha * (returns - values[state_])\n",
    "    return trajectory, [returns] * (len(trajectory) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Example 6.2 left\n",
    "def compute_state_value():\n",
    "    episodes = [0, 1, 10, 100]\n",
    "    current_values = np.copy(VALUES)\n",
    "    plt.figure(1)\n",
    "    for i in range(episodes[-1] + 1):\n",
    "        if i in episodes:\n",
    "            plt.plot(current_values, label=str(i) + ' episodes')\n",
    "        temporal_difference(current_values)\n",
    "    plt.plot(TRUE_VALUE, label='true values')\n",
    "    plt.xlabel('state')\n",
    "    plt.ylabel('estimated value')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Example 6.2 right\n",
    "def rms_error():\n",
    "    # Same alpha value can appear in both arrays\n",
    "    td_alphas = [0.15, 0.1, 0.05]\n",
    "    mc_alphas = [0.01, 0.02, 0.03, 0.04]\n",
    "    episodes = 100 + 1\n",
    "    runs = 100\n",
    "    for i, alpha in enumerate(td_alphas + mc_alphas):\n",
    "        total_errors = np.zeros(episodes)\n",
    "        if i < len(td_alphas):\n",
    "            method = 'TD'\n",
    "            linestyle = 'solid'\n",
    "        else:\n",
    "            method = 'MC'\n",
    "            linestyle = 'dashdot'\n",
    "        for r in tqdm(range(runs)):\n",
    "            errors = []\n",
    "            current_values = np.copy(VALUES)\n",
    "            for i in range(0, episodes):\n",
    "                errors.append(np.sqrt(np.sum(np.power(TRUE_VALUE - current_values, 2)) / 5.0))\n",
    "                if method == 'TD':\n",
    "                    temporal_difference(current_values, alpha=alpha)\n",
    "                else:\n",
    "                    monte_carlo(current_values, alpha=alpha)\n",
    "            total_errors += np.asarray(errors)\n",
    "        total_errors /= runs\n",
    "        plt.plot(total_errors, linestyle=linestyle, label=method + ', alpha = %.02f' % (alpha))\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('RMS')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Figure 6.2\n",
    "# @method: 'TD' or 'MC'\n",
    "def batch_updating(method, episodes, alpha=0.001):\n",
    "    # perform 100 independent runs\n",
    "    runs = 100\n",
    "    total_errors = np.zeros(episodes)\n",
    "    for r in tqdm(range(0, runs)):\n",
    "        current_values = np.copy(VALUES)\n",
    "        errors = []\n",
    "        # track shown trajectories and reward/return sequences\n",
    "        trajectories = []\n",
    "        rewards = []\n",
    "        for ep in range(episodes):\n",
    "            if method == 'TD':\n",
    "                trajectory_, rewards_ = temporal_difference(current_values, batch=True)\n",
    "            else:\n",
    "                trajectory_, rewards_ = monte_carlo(current_values, batch=True)\n",
    "            trajectories.append(trajectory_)\n",
    "            rewards.append(rewards_)\n",
    "            while True:\n",
    "                # keep feeding our algorithm with trajectories seen so far until state value function converges\n",
    "                updates = np.zeros(7)\n",
    "                for trajectory_, rewards_ in zip(trajectories, rewards):\n",
    "                    for i in range(0, len(trajectory_) - 1):\n",
    "                        if method == 'TD':\n",
    "                            updates[trajectory_[i]] += rewards_[i] + current_values[trajectory_[i + 1]] - current_values[trajectory_[i]]\n",
    "                        else:\n",
    "                            updates[trajectory_[i]] += rewards_[i] - current_values[trajectory_[i]]\n",
    "                updates *= alpha\n",
    "                if np.sum(np.abs(updates)) < 1e-3:\n",
    "                    break\n",
    "                # perform batch updating\n",
    "                current_values += updates\n",
    "            # calculate rms error\n",
    "            errors.append(np.sqrt(np.sum(np.power(current_values - TRUE_VALUE, 2)) / 5.0))\n",
    "        total_errors += np.asarray(errors)\n",
    "    total_errors /= runs\n",
    "    return total_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def example_6_2():\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    compute_state_value()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    rms_error()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def figure_6_2():\n",
    "    episodes = 100 + 1\n",
    "    td_erros = batch_updating('TD', episodes)\n",
    "    mc_erros = batch_updating('MC', episodes)\n",
    "\n",
    "    plt.plot(td_erros, label='TD')\n",
    "    plt.plot(mc_erros, label='MC')\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('RMS error')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "example_6_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "figure_6_2()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
