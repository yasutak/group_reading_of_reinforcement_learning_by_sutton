{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# # of states except for terminal states\n",
    "N_STATES = 1000\n",
    "\n",
    "# all states\n",
    "STATES = np.arange(1, N_STATES + 1)\n",
    "\n",
    "# start from a central state\n",
    "START_STATE = 500\n",
    "\n",
    "# terminal states\n",
    "END_STATES = [0, N_STATES + 1]\n",
    "\n",
    "# possible actions\n",
    "ACTION_LEFT = -1\n",
    "ACTION_RIGHT = 1\n",
    "ACTIONS = [ACTION_LEFT, ACTION_RIGHT]\n",
    "\n",
    "# maximum stride for an action\n",
    "STEP_RANGE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_true_value():\n",
    "    # true state value, just a promising guess\n",
    "    true_value = np.arange(-1001, 1003, 2) / 1001.0\n",
    "\n",
    "    # Dynamic programming to find the true state values, based on the promising guess above\n",
    "    # Assume all rewards are 0, given that we have already given value -1 and 1 to terminal states\n",
    "    while True:\n",
    "        old_value = np.copy(true_value)\n",
    "        for state in STATES:\n",
    "            true_value[state] = 0\n",
    "            for action in ACTIONS:\n",
    "                for step in range(1, STEP_RANGE + 1):\n",
    "                    step *= action\n",
    "                    next_state = state + step\n",
    "                    next_state = max(min(next_state, N_STATES + 1), 0)\n",
    "                    # asynchronous update for faster convergence\n",
    "                    true_value[state] += 1.0 / (2 * STEP_RANGE) * true_value[next_state]\n",
    "        error = np.sum(np.abs(old_value - true_value))\n",
    "        if error < 1e-2:\n",
    "            break\n",
    "    # correct the state value for terminal states to 0\n",
    "    true_value[0] = true_value[-1] = 0\n",
    "\n",
    "    return true_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# take an @action at @state, return new state and reward for this transition\n",
    "def step(state, action):\n",
    "    step = np.random.randint(1, STEP_RANGE + 1)\n",
    "    step *= action\n",
    "    state += step\n",
    "    state = max(min(state, N_STATES + 1), 0)\n",
    "    if state == 0:\n",
    "        reward = -1\n",
    "    elif state == N_STATES + 1:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "    return state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# get an action, following random policy\n",
    "def get_action():\n",
    "    if np.random.binomial(1, 0.5) == 1:\n",
    "        return 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# a wrapper class for aggregation value function\n",
    "class ValueFunction:\n",
    "    # @num_of_groups: # of aggregations\n",
    "    def __init__(self, num_of_groups):\n",
    "        self.num_of_groups = num_of_groups\n",
    "        self.group_size = N_STATES // num_of_groups\n",
    "\n",
    "        # thetas\n",
    "        self.params = np.zeros(num_of_groups)\n",
    "\n",
    "    # get the value of @state\n",
    "    def value(self, state):\n",
    "        if state in END_STATES:\n",
    "            return 0\n",
    "        group_index = (state - 1) // self.group_size\n",
    "        return self.params[group_index]\n",
    "\n",
    "    # update parameters\n",
    "    # @delta: step size * (target - old estimation)\n",
    "    # @state: state of current sample\n",
    "    def update(self, delta, state):\n",
    "        group_index = (state - 1) // self.group_size\n",
    "        self.params[group_index] += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# a wrapper class for tile coding value function\n",
    "class TilingsValueFunction:\n",
    "    # @num_of_tilings: # of tilings\n",
    "    # @tileWidth: each tiling has several tiles, this parameter specifies the width of each tile\n",
    "    # @tilingOffset: specifies how tilings are put together\n",
    "    def __init__(self, numOfTilings, tileWidth, tilingOffset):\n",
    "        self.numOfTilings = numOfTilings\n",
    "        self.tileWidth = tileWidth\n",
    "        self.tilingOffset = tilingOffset\n",
    "\n",
    "        # To make sure that each sate is covered by same number of tiles,\n",
    "        # we need one more tile for each tiling\n",
    "        self.tilingSize = N_STATES // tileWidth + 1\n",
    "\n",
    "        # weight for each tile\n",
    "        self.params = np.zeros((self.numOfTilings, self.tilingSize))\n",
    "\n",
    "        # For performance, only track the starting position for each tiling\n",
    "        # As we have one more tile for each tiling, the starting position will be negative\n",
    "        self.tilings = np.arange(-tileWidth + 1, 0, tilingOffset)\n",
    "\n",
    "    # get the value of @state\n",
    "    def value(self, state):\n",
    "        stateValue = 0.0\n",
    "        # go through all the tilings\n",
    "        for tilingIndex in range(0, len(self.tilings)):\n",
    "            # find the active tile in current tiling\n",
    "            tileIndex = (state - self.tilings[tilingIndex]) // self.tileWidth\n",
    "            stateValue += self.params[tilingIndex, tileIndex]\n",
    "        return stateValue\n",
    "\n",
    "    # update parameters\n",
    "    # @delta: step size * (target - old estimation)\n",
    "    # @state: state of current sample\n",
    "    def update(self, delta, state):\n",
    "\n",
    "        # each state is covered by same number of tilings\n",
    "        # so the delta should be divided equally into each tiling (tile)\n",
    "        delta /= self.numOfTilings\n",
    "\n",
    "        # go through all the tilings\n",
    "        for tilingIndex in range(0, len(self.tilings)):\n",
    "            # find the active tile in current tiling\n",
    "            tileIndex = (state - self.tilings[tilingIndex]) // self.tileWidth\n",
    "            self.params[tilingIndex, tileIndex] += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# a wrapper class for polynomial / Fourier -based value function\n",
    "POLYNOMIAL_BASES = 0\n",
    "FOURIER_BASES = 1\n",
    "class BasesValueFunction:\n",
    "    # @order: # of bases, each function also has one more constant parameter (called bias in machine learning)\n",
    "    # @type: polynomial bases or Fourier bases\n",
    "    def __init__(self, order, type):\n",
    "        self.order = order\n",
    "        self.weights = np.zeros(order + 1)\n",
    "\n",
    "        # set up bases function\n",
    "        self.bases = []\n",
    "        if type == POLYNOMIAL_BASES:\n",
    "            for i in range(0, order + 1):\n",
    "                self.bases.append(lambda s, i=i: pow(s, i))\n",
    "        elif type == FOURIER_BASES:\n",
    "            for i in range(0, order + 1):\n",
    "                self.bases.append(lambda s, i=i: np.cos(i * np.pi * s))\n",
    "\n",
    "    # get the value of @state\n",
    "    def value(self, state):\n",
    "        # map the state space into [0, 1]\n",
    "        state /= float(N_STATES)\n",
    "        # get the feature vector\n",
    "        feature = np.asarray([func(state) for func in self.bases])\n",
    "        return np.dot(self.weights, feature)\n",
    "\n",
    "    def update(self, delta, state):\n",
    "        # map the state space into [0, 1]\n",
    "        state /= float(N_STATES)\n",
    "        # get derivative value\n",
    "        derivative_value = np.asarray([func(state) for func in self.bases])\n",
    "        self.weights += delta * derivative_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# gradient Monte Carlo algorithm\n",
    "# @value_function: an instance of class ValueFunction\n",
    "# @alpha: step size\n",
    "# @distribution: array to store the distribution statistics\n",
    "def gradient_monte_carlo(value_function, alpha, distribution=None):\n",
    "    state = START_STATE\n",
    "    trajectory = [state]\n",
    "\n",
    "    # We assume gamma = 1, so return is just the same as the latest reward\n",
    "    reward = 0.0\n",
    "    while state not in END_STATES:\n",
    "        action = get_action()\n",
    "        next_state, reward = step(state, action)\n",
    "        trajectory.append(next_state)\n",
    "        state = next_state\n",
    "\n",
    "    # Gradient update for each state in this trajectory\n",
    "    for state in trajectory[:-1]:\n",
    "        delta = alpha * (reward - value_function.value(state))\n",
    "        value_function.update(delta, state)\n",
    "        if distribution is not None:\n",
    "            distribution[state] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# semi-gradient n-step TD algorithm\n",
    "# @valueFunction: an instance of class ValueFunction\n",
    "# @n: # of steps\n",
    "# @alpha: step size\n",
    "def semi_gradient_temporal_difference(value_function, n, alpha):\n",
    "    # initial starting state\n",
    "    state = START_STATE\n",
    "\n",
    "    # arrays to store states and rewards for an episode\n",
    "    # space isn't a major consideration, so I didn't use the mod trick\n",
    "    states = [state]\n",
    "    rewards = [0]\n",
    "\n",
    "    # track the time\n",
    "    time = 0\n",
    "\n",
    "    # the length of this episode\n",
    "    T = float('inf')\n",
    "    while True:\n",
    "        # go to next time step\n",
    "        time += 1\n",
    "\n",
    "        if time < T:\n",
    "            # choose an action randomly\n",
    "            action = get_action()\n",
    "            next_state, reward = step(state, action)\n",
    "\n",
    "            # store new state and new reward\n",
    "            states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if next_state in END_STATES:\n",
    "                T = time\n",
    "\n",
    "        # get the time of the state to update\n",
    "        update_time = time - n\n",
    "        if update_time >= 0:\n",
    "            returns = 0.0\n",
    "            # calculate corresponding rewards\n",
    "            for t in range(update_time + 1, min(T, update_time + n) + 1):\n",
    "                returns += rewards[t]\n",
    "            # add state value to the return\n",
    "            if update_time + n <= T:\n",
    "                returns += value_function.value(states[update_time + n])\n",
    "            state_to_update = states[update_time]\n",
    "            # update the value function\n",
    "            if not state_to_update in END_STATES:\n",
    "                delta = alpha * (returns - value_function.value(state_to_update))\n",
    "                value_function.update(delta, state_to_update)\n",
    "        if update_time == T - 1:\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Figure 9.1, gradient Monte Carlo algorithm\n",
    "def figure_9_1(true_value):\n",
    "    episodes = int(1e5)\n",
    "    alpha = 2e-5\n",
    "\n",
    "    # we have 10 aggregations in this example, each has 100 states\n",
    "    value_function = ValueFunction(10)\n",
    "    distribution = np.zeros(N_STATES + 2)\n",
    "    for ep in tqdm(range(episodes)):\n",
    "        gradient_monte_carlo(value_function, alpha, distribution)\n",
    "\n",
    "    distribution /= np.sum(distribution)\n",
    "    state_values = [value_function.value(i) for i in STATES]\n",
    "\n",
    "    plt.figure(figsize=(10, 20))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(STATES, state_values, label='Approximate MC value')\n",
    "    plt.plot(STATES, true_value[1: -1], label='True value')\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(STATES, distribution[1: -1], label='State distribution')\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Distribution')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# semi-gradient TD on 1000-state random walk\n",
    "def figure_9_2_left(true_value):\n",
    "    episodes = int(1e5)\n",
    "    alpha = 2e-4\n",
    "    value_function = ValueFunction(10)\n",
    "    for ep in tqdm(range(episodes)):\n",
    "        semi_gradient_temporal_difference(value_function, 1, alpha)\n",
    "\n",
    "    stateValues = [value_function.value(i) for i in STATES]\n",
    "    plt.plot(STATES, stateValues, label='Approximate TD value')\n",
    "    plt.plot(STATES, true_value[1: -1], label='True value')\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# different alphas and steps for semi-gradient TD\n",
    "def figure_9_2_right(true_value):\n",
    "    # all possible steps\n",
    "    steps = np.power(2, np.arange(0, 10))\n",
    "\n",
    "    # all possible alphas\n",
    "    alphas = np.arange(0, 1.1, 0.1)\n",
    "\n",
    "    # each run has 10 episodes\n",
    "    episodes = 10\n",
    "\n",
    "    # perform 100 independent runs\n",
    "    runs = 100\n",
    "\n",
    "    # track the errors for each (step, alpha) combination\n",
    "    errors = np.zeros((len(steps), len(alphas)))\n",
    "    for run in tqdm(range(runs)):\n",
    "        for step_ind, step in zip(range(len(steps)), steps):\n",
    "            for alpha_ind, alpha in zip(range(len(alphas)), alphas):\n",
    "                # we have 20 aggregations in this example\n",
    "                value_function = ValueFunction(20)\n",
    "                for ep in range(0, episodes):\n",
    "                    semi_gradient_temporal_difference(value_function, step, alpha)\n",
    "                    # calculate the RMS error\n",
    "                    state_value = np.asarray([value_function.value(i) for i in STATES])\n",
    "                    errors[step_ind, alpha_ind] += np.sqrt(np.sum(np.power(state_value - true_value[1: -1], 2)) / N_STATES)\n",
    "    # take average\n",
    "    errors /= episodes * runs\n",
    "    # truncate the error\n",
    "    for i in range(len(steps)):\n",
    "        plt.plot(alphas, errors[i, :], label='n = ' + str(steps[i]))\n",
    "    plt.xlabel('alpha')\n",
    "    plt.ylabel('RMS error')\n",
    "    plt.ylim([0.25, 0.55])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def figure_9_2(true_value):\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    figure_9_2_left(true_value)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    figure_9_2_right(true_value)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Figure 9.5, Fourier basis and polynomials\n",
    "def figure_9_5(true_value):\n",
    "    # my machine can only afford 1 run\n",
    "    runs = 1\n",
    "\n",
    "    episodes = 5000\n",
    "\n",
    "    # # of bases\n",
    "    orders = [5, 10, 20]\n",
    "\n",
    "    alphas = [1e-4, 5e-5]\n",
    "    labels = [['polynomial basis'] * 3, ['fourier basis'] * 3]\n",
    "\n",
    "    # track errors for each episode\n",
    "    errors = np.zeros((len(alphas), len(orders), episodes))\n",
    "    for run in range(runs):\n",
    "        for i in range(len(orders)):\n",
    "            value_functions = [BasesValueFunction(orders[i], POLYNOMIAL_BASES), BasesValueFunction(orders[i], FOURIER_BASES)]\n",
    "            for j in range(len(value_functions)):\n",
    "                for episode in tqdm(range(episodes)):\n",
    "\n",
    "                    # gradient Monte Carlo algorithm\n",
    "                    gradient_monte_carlo(value_functions[j], alphas[j])\n",
    "\n",
    "                    # get state values under current value function\n",
    "                    state_values = [value_functions[j].value(state) for state in STATES]\n",
    "\n",
    "                    # get the root-mean-squared error\n",
    "                    errors[j, i, episode] += np.sqrt(np.mean(np.power(true_value[1: -1] - state_values, 2)))\n",
    "\n",
    "    # average over independent runs\n",
    "    errors /= runs\n",
    "\n",
    "    for i in range(len(alphas)):\n",
    "        for j in range(len(orders)):\n",
    "            plt.plot(errors[i, j, :], label='%s order = %d' % (labels[i][j], orders[j]))\n",
    "    plt.xlabel('Episodes')\n",
    "    # The book plots RMSVE, which is RMSE weighted by a state distribution\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Figure 9.10, it will take quite a while\n",
    "def figure_9_10(true_value):\n",
    "\n",
    "    # My machine can only afford one run, thus the curve isn't so smooth\n",
    "    runs = 1\n",
    "\n",
    "    # number of episodes\n",
    "    episodes = 5000\n",
    "\n",
    "    num_of_tilings = 50\n",
    "\n",
    "    # each tile will cover 200 states\n",
    "    tile_width = 200\n",
    "\n",
    "    # how to put so many tilings\n",
    "    tiling_offset = 4\n",
    "\n",
    "    labels = ['tile coding (50 tilings)', 'state aggregation (one tiling)']\n",
    "\n",
    "    # track errors for each episode\n",
    "    errors = np.zeros((len(labels), episodes))\n",
    "    for run in range(runs):\n",
    "        # initialize value functions for multiple tilings and single tiling\n",
    "        value_functions = [TilingsValueFunction(num_of_tilings, tile_width, tiling_offset),\n",
    "                         ValueFunction(N_STATES // tile_width)]\n",
    "        for i in range(len(value_functions)):\n",
    "            for episode in tqdm(range(episodes)):\n",
    "                # I use a changing alpha according to the episode instead of a small fixed alpha\n",
    "                # With a small fixed alpha, I don't think 5000 episodes is enough for so many\n",
    "                # parameters in multiple tilings.\n",
    "                # The asymptotic performance for single tiling stays unchanged under a changing alpha,\n",
    "                # however the asymptotic performance for multiple tilings improves significantly\n",
    "                alpha = 1.0 / (episode + 1)\n",
    "\n",
    "                # gradient Monte Carlo algorithm\n",
    "                gradient_monte_carlo(value_functions[i], alpha)\n",
    "\n",
    "                # get state values under current value function\n",
    "                state_values = [value_functions[i].value(state) for state in STATES]\n",
    "\n",
    "                # get the root-mean-squared error\n",
    "                errors[i][episode] += np.sqrt(np.mean(np.power(true_value[1: -1] - state_values, 2)))\n",
    "\n",
    "    # average over independent runs\n",
    "    errors /= runs\n",
    "\n",
    "    for i in range(0, len(labels)):\n",
    "        plt.plot(errors[i], label=labels[i])\n",
    "    plt.xlabel('Episodes')\n",
    "    # The book plots RMSVE, which is RMSE weighted by a state distribution\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_value = compute_true_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "figure_9_1(true_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "figure_9_2(true_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "figure_9_5(true_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_9_10(true_value)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
